{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X56HkhjrqARU"
      },
      "source": [
        "# Energieverbrauchsvorhersage - Stacking Experiment\n",
        "\n",
        "\n",
        "## Problemstellung\n",
        "Das Ziel ist die Vorhersage des Energieverbrauchs unter Verwendung von verschiedenen Deep Learning-Modellen und Stacking-Techniken, um dem gegeben Extrapolationsproblem entgegenzuwirken.\n",
        "\n",
        "## Daten\n",
        "- **energy_consumed:** Die Zielvariable variiert von ca. 0.00000005 bis 0.00032 kWh\n",
        "- **num_num_features:** Die Anzahl der numerischen und kategorischen Features reichen von 0 bis 7 bzw. 0 bis 14\n",
        "- **num_cat_features:** Die Anzahl kategorischen Features reichen von 0 bis 14\n",
        "- **number_of_instances:** Die Größe der Datensätze variiert von 9000 bis 320000 Instanzen\n",
        "- **model:** Die verwendeten Algorithmen sind kategorisch dargestellt, mit Werten von 0 bis 4\n",
        "\n",
        "## Vorgehen:\n",
        "\n",
        "### Datenanalyse und Vorverarbeitung\n",
        "Hier führen wir die Exploration und Vorverarbeitung der Daten durch. Dies beinhaltet überwiegend die Kodierung kategorischer Features.\n",
        "\n",
        "### Modellbildung\n",
        "In diesem Abschnitt bauen wir verschiedene Deep Learning-Modelle und verwenden die Stacking-Technik, um die Vorhersagegenauigkeit zu erhöhen. Wir verwenden die folgenden Schritte:\n",
        "\n",
        "- Schritt 1: Baseline Modelle trainieren\n",
        "- Schritt 1: Metamodell trainieren mittels der Baseline Vorhersagen\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt3507GZtBdr"
      },
      "source": [
        "## Vorbereitung"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jXvUO2LtGV5"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9P6niB9Lu_0v",
        "outputId": "b5aa3d41-79c0-4355-f8ed-a7c7f3da9956"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Jan 22 20:18:58 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla V100-SXM2-16GB           Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0              26W / 300W |      2MiB / 16384MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Hardwareinfo\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwSooAKXuu4F"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T6hbs5Huu6m",
        "outputId": "e60e4427-66de-4015-9877-328b17f28706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/hollerith-energy-ml/data/energy_predcition_training-data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0ZRVTX2t0t6"
      },
      "source": [
        "### Daten vorbereiten\n",
        "- Codierung der kategorischen Features\n",
        "- Skalierung der numerischen Features\n",
        "- Aufteilung in Trainings- und Testsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ojaXgTjttoq"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop('energy_consumed', axis=1)\n",
        "y = df['energy_consumed']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "num_features = ['num_num_features', 'num_cat_features', 'number_of_instances']\n",
        "cat_features = ['model']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybl-8SL2vvwh",
        "outputId": "fe71ac0d-cbc1-4a6b-fe3f-eb859e6e839b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((10040, 8), (2510, 8))"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing importü StandardScaler, OneHotEncoder\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), num_features),\n",
        "        ('cat', OneHotEncoder(), cat_features)\n",
        "    ])\n",
        "\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "X_train_processed.shape, X_test_processed.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B7IyVkzyMbE"
      },
      "source": [
        "## Hilfsfunktion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-cfY4JQyO_k"
      },
      "outputs": [],
      "source": [
        "def format_energy(value_in_kwh):\n",
        "    units = [\n",
        "        (\"TWh\", 1e-9),\n",
        "        (\"GWh\", 1e-6),\n",
        "        (\"MWh\", 1e-3),\n",
        "        (\"kWh\", 1),\n",
        "        (\"Wh\", 1e3),\n",
        "        (\"mWh\", 1e6),\n",
        "        (\"µWh\", 1e9),\n",
        "        (\"nWh\", 1e12),\n",
        "        (\"pWh\", 1e15)\n",
        "    ]\n",
        "\n",
        "    for unit, factor in units:\n",
        "        if value_in_kwh * factor >= 1:\n",
        "            return f\"{value_in_kwh * factor:.2f} {unit}\"\n",
        "\n",
        "    return \"0 µWh\"\n",
        "\n",
        "model_category_mapping = {\n",
        "    'Decision Tree': 0,\n",
        "    'Gaussian Naive Bayes': 1,\n",
        "    'K-Nearest Neighbors': 2,\n",
        "    'Logistic Regression': 3,\n",
        "    'Random Forest': 4\n",
        "}\n",
        "\n",
        "def predict_and_display_energy_consumption(encoder, num_num_features, num_cat_features, number_of_instances):\n",
        "    input_data = {\n",
        "        'num_num_features': [num_num_features] * 5,\n",
        "        'num_cat_features': [num_cat_features] * 5,\n",
        "        'number_of_instances': [number_of_instances] * 5,\n",
        "        'model': list(range(5))\n",
        "    }\n",
        "\n",
        "    input_df = pd.DataFrame(input_data)\n",
        "    input_processed = encoder.transform(input_df)\n",
        "\n",
        "    all_models = {**base_models, 'Stacking Ensemble': stacking_model}\n",
        "\n",
        "    predictions = {}\n",
        "    for model_category in range(5):\n",
        "        for name, model in all_models.items():\n",
        "            prediction = model.predict(input_processed[model_category].reshape(1, -1))[0]\n",
        "            converted_prediction = format_energy(prediction)\n",
        "            model_category_name = list(model_category_mapping.keys())[list(model_category_mapping.values()).index(model_category)]\n",
        "            predictions[(model_category_name, name)] = converted_prediction\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rfeGiuDwJ4a"
      },
      "source": [
        "## Experiment 1\n",
        "\n",
        "**Basis-Modelle:**\n",
        "- Random Forest Regressor: Da es gute Ergebnisse auf den Trainingsdaten leiferte\n",
        "- Gradient Boosting Regressor: da gut für Regressionsaufgaben und kann nichtlineare Muster erkennen\n",
        "- Lineare Regression: um die Extrapolationsfähigkeit zu nutzen\n",
        "\n",
        "**Meta-Modell:**\n",
        "- Lineare Regression: Vorhersagen gewichten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HJJ4qvrwFD3",
        "outputId": "9cde02a8-ac86-4323-a790-8fcf00ab0c7b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Random Forest': 4.140615694231319e-06,\n",
              " 'Gradient Boosting': 5.288152399900951e-06,\n",
              " 'Linear Regression': 2.793982251057454e-05}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "\n",
        "base_models = {\n",
        "    \"Random Forest\": RandomForestRegressor(random_state=42),\n",
        "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
        "    \"Linear Regression\": LinearRegression()\n",
        "}\n",
        "\n",
        "# Kreuzvalidierung einrichten\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Bewertung der Basis-Modelle\n",
        "model_scores = {}\n",
        "for name, model in base_models.items():\n",
        "    cv_scores = cross_val_score(model, X_train_processed, y_train, cv=kf, scoring='neg_mean_squared_error')\n",
        "    model_scores[name] = np.mean(np.sqrt(-cv_scores))\n",
        "\n",
        "model_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lIVjapaxayu",
        "outputId": "2f33eaca-b307-4fe8-8c65-21b0076e0f99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Random Forest': {'bootstrap': True,\n",
              "  'ccp_alpha': 0.0,\n",
              "  'criterion': 'squared_error',\n",
              "  'max_depth': 10,\n",
              "  'max_features': 1.0,\n",
              "  'max_leaf_nodes': None,\n",
              "  'max_samples': None,\n",
              "  'min_impurity_decrease': 0.0,\n",
              "  'min_samples_leaf': 1,\n",
              "  'min_samples_split': 2,\n",
              "  'min_weight_fraction_leaf': 0.0,\n",
              "  'n_estimators': 150,\n",
              "  'n_jobs': None,\n",
              "  'oob_score': False,\n",
              "  'random_state': 42,\n",
              "  'verbose': 0,\n",
              "  'warm_start': False},\n",
              " 'Gradient Boosting': {'alpha': 0.9,\n",
              "  'ccp_alpha': 0.0,\n",
              "  'criterion': 'friedman_mse',\n",
              "  'init': None,\n",
              "  'learning_rate': 0.2,\n",
              "  'loss': 'squared_error',\n",
              "  'max_depth': 5,\n",
              "  'max_features': None,\n",
              "  'max_leaf_nodes': None,\n",
              "  'min_impurity_decrease': 0.0,\n",
              "  'min_samples_leaf': 1,\n",
              "  'min_samples_split': 2,\n",
              "  'min_weight_fraction_leaf': 0.0,\n",
              "  'n_estimators': 150,\n",
              "  'n_iter_no_change': None,\n",
              "  'random_state': 42,\n",
              "  'subsample': 1.0,\n",
              "  'tol': 0.0001,\n",
              "  'validation_fraction': 0.1,\n",
              "  'verbose': 0,\n",
              "  'warm_start': False},\n",
              " 'Linear Regression': {'copy_X': True,\n",
              "  'fit_intercept': True,\n",
              "  'n_jobs': None,\n",
              "  'positive': False}}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Hyperparameter\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    \"Random Forest\": {\n",
        "        'n_estimators': [50, 100, 150],\n",
        "        'max_depth': [None, 5, 10],\n",
        "        'min_samples_split': [2, 4]\n",
        "    },\n",
        "    \"Gradient Boosting\": {\n",
        "        'n_estimators': [50, 100, 150],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7]\n",
        "    }\n",
        "}\n",
        "\n",
        "tuned_models = {}\n",
        "for name, model in base_models.items():\n",
        "    if name != \"Linear Regression\":\n",
        "        grid_search = GridSearchCV(model, param_grid[name], cv=kf, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "        grid_search.fit(X_train_processed, y_train)\n",
        "        tuned_models[name] = grid_search.best_estimator_\n",
        "    else:\n",
        "        tuned_models[name] = model\n",
        "\n",
        "tuned_model_details = {name: model.get_params() for name, model in tuned_models.items()}\n",
        "tuned_model_details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "B8I8zNJOx6g7",
        "outputId": "7b211d10-59d4-4d38-f80d-104994dd35c4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StackingRegressor(estimators=[(&#x27;Random Forest&#x27;,\n",
              "                               RandomForestRegressor(max_depth=10,\n",
              "                                                     n_estimators=150,\n",
              "                                                     random_state=42)),\n",
              "                              (&#x27;Gradient Boosting&#x27;,\n",
              "                               GradientBoostingRegressor(learning_rate=0.2,\n",
              "                                                         max_depth=5,\n",
              "                                                         n_estimators=150,\n",
              "                                                         random_state=42)),\n",
              "                              (&#x27;Linear Regression&#x27;, LinearRegression())],\n",
              "                  final_estimator=LinearRegression())</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StackingRegressor</label><div class=\"sk-toggleable__content\"><pre>StackingRegressor(estimators=[(&#x27;Random Forest&#x27;,\n",
              "                               RandomForestRegressor(max_depth=10,\n",
              "                                                     n_estimators=150,\n",
              "                                                     random_state=42)),\n",
              "                              (&#x27;Gradient Boosting&#x27;,\n",
              "                               GradientBoostingRegressor(learning_rate=0.2,\n",
              "                                                         max_depth=5,\n",
              "                                                         n_estimators=150,\n",
              "                                                         random_state=42)),\n",
              "                              (&#x27;Linear Regression&#x27;, LinearRegression())],\n",
              "                  final_estimator=LinearRegression())</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>Random Forest</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=10, n_estimators=150, random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>Gradient Boosting</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.2, max_depth=5, n_estimators=150,\n",
              "                          random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>Linear Regression</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "StackingRegressor(estimators=[('Random Forest',\n",
              "                               RandomForestRegressor(max_depth=10,\n",
              "                                                     n_estimators=150,\n",
              "                                                     random_state=42)),\n",
              "                              ('Gradient Boosting',\n",
              "                               GradientBoostingRegressor(learning_rate=0.2,\n",
              "                                                         max_depth=5,\n",
              "                                                         n_estimators=150,\n",
              "                                                         random_state=42)),\n",
              "                              ('Linear Regression', LinearRegression())],\n",
              "                  final_estimator=LinearRegression())"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import StackingRegressor\n",
        "\n",
        "# Basis-Modelle mit gefundenen Hyperparametern\n",
        "base_models['Random Forest'].set_params(**tuned_model_details['Random Forest'])\n",
        "base_models['Gradient Boosting'].set_params(**tuned_model_details['Gradient Boosting'])\n",
        "\n",
        "for model in base_models.values():\n",
        "    model.fit(X_train_processed, y_train)\n",
        "\n",
        "# Stacking-Modell\n",
        "stacking_model = StackingRegressor(\n",
        "    estimators=[(name, model) for name, model in base_models.items()],\n",
        "    final_estimator=LinearRegression()\n",
        ")\n",
        "\n",
        "stacking_model.fit(X_train_processed, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDdAeFQMzYWZ",
        "outputId": "b80dee44-28f2-45fc-8185-8b67b9c1d5e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE des Stacking-Modell: 3.7408045445241383e-06\n",
            "R^2 des Stacking-Modell: 0.9906563965372944\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "y_pred = stacking_model.predict(X_test_processed)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f\"RMSE des Stacking-Modell: {rmse}\")\n",
        "print(f\"R^2 des Stacking-Modell: {r2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 705
        },
        "id": "YFw2wsPsdI-v",
        "outputId": "a3a3e47a-9727-4c51-c6bd-b4827178891b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature-Wichtigkeiten im Random Forest:\n",
            "Feature 8: 0.3525\n",
            "Feature 3: 0.3072\n",
            "Feature 1: 0.2881\n",
            "Feature 2: 0.0279\n",
            "Feature 7: 0.0230\n",
            "Feature 4: 0.0012\n",
            "Feature 5: 0.0001\n",
            "Feature 6: 0.0001\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Analyse der Feature-Wichtigkeit für Random Forest\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "rf_feature_importance = base_models['Random Forest'].feature_importances_\n",
        "indices = np.argsort(rf_feature_importance)[::-1]\n",
        "\n",
        "print(\"Feature-Wichtigkeiten im Random Forest:\")\n",
        "for i, index in enumerate(indices):\n",
        "    feature_name = f\"Feature {index + 1}\"\n",
        "    importance = rf_feature_importance[index]\n",
        "    print(f\"{feature_name}: {importance:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.title(\"Feature-Wichtigkeiten im Random Forest\")\n",
        "plt.bar(range(X_train_processed.shape[1]), rf_feature_importance[indices],\n",
        "       color=\"r\", align=\"center\")\n",
        "plt.xticks(range(X_train_processed.shape[1]), indices)\n",
        "plt.xlim([-1, X_train_processed.shape[1]])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5KKl1oZ3Jq-"
      },
      "source": [
        "### Vorhersagen auf unbekannte Daten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQAqVONt3TZ5",
        "outputId": "d7ccf084-33d1-4ba9-ae22-1c08c587dbfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modell Kategorie Decision Tree, Random Forest: 2.35 mWh\n",
            "Modell Kategorie Decision Tree, Gradient Boosting: 2.16 mWh\n",
            "Modell Kategorie Decision Tree, Linear Regression: 116.04 mWh\n",
            "Modell Kategorie Decision Tree, Stacking Ensemble: 2.21 mWh\n",
            "\n",
            "Modell Kategorie Gaussian Naive Bayes, Random Forest: 198.01 µWh\n",
            "Modell Kategorie Gaussian Naive Bayes, Gradient Boosting: 25.52 µWh\n",
            "Modell Kategorie Gaussian Naive Bayes, Linear Regression: 112.85 mWh\n",
            "Modell Kategorie Gaussian Naive Bayes, Stacking Ensemble: 74.75 µWh\n",
            "\n",
            "Modell Kategorie K-Nearest Neighbors, Random Forest: 148.85 µWh\n",
            "Modell Kategorie K-Nearest Neighbors, Gradient Boosting: 0 µWh\n",
            "Modell Kategorie K-Nearest Neighbors, Linear Regression: 114.67 mWh\n",
            "Modell Kategorie K-Nearest Neighbors, Stacking Ensemble: 0 µWh\n",
            "\n",
            "Modell Kategorie Logistic Regression, Random Forest: 18.24 mWh\n",
            "Modell Kategorie Logistic Regression, Gradient Boosting: 16.59 mWh\n",
            "Modell Kategorie Logistic Regression, Linear Regression: 127.79 mWh\n",
            "Modell Kategorie Logistic Regression, Stacking Ensemble: 16.69 mWh\n",
            "\n",
            "Modell Kategorie Random Forest, Random Forest: 46.82 mWh\n",
            "Modell Kategorie Random Forest, Gradient Boosting: 42.54 mWh\n",
            "Modell Kategorie Random Forest, Linear Regression: 178.64 mWh\n",
            "Modell Kategorie Random Forest, Stacking Ensemble: 42.74 mWh\n"
          ]
        }
      ],
      "source": [
        "final_predictions = predict_and_display_energy_consumption(\n",
        "    encoder=preprocessor,\n",
        "    num_num_features=5000,\n",
        "    num_cat_features=1300,\n",
        "    number_of_instances=10000\n",
        ")\n",
        "\n",
        "output_lines = [f\"Modell Kategorie {model_category}, {model_name}: {prediction}\" for (model_category, model_name), prediction in final_predictions.items()]\n",
        "output_paragraphs = ['\\n'.join(output_lines[i:i+4]) for i in range(0, len(output_lines), 4)]\n",
        "print('\\n\\n'.join(output_paragraphs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkK0xz-heZsG"
      },
      "source": [
        "## Experiment 2\n",
        "\n",
        "**Basis-Modelle:**\n",
        "- Random Forest Regressor: Da es gute Ergebnisse auf den Trainingsdaten leiferte\n",
        "- Gradient Boosting Regressor: da gut für Regressionsaufgaben und kann nichtlineare Muster erkennen\n",
        "- Lineare Regression: um die Extrapolationsfähigkeit zu nutzen\n",
        "- Ridge\n",
        "\n",
        "**Meta-Modell:**\n",
        "- H2O"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoqhgCSwlcr8",
        "outputId": "25c7ec8a-fbc1-4067-f881-85edb0e6bb70"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Random Forest': 4.140615694231319e-06,\n",
              " 'Gradient Boosting': 5.288152399900951e-06,\n",
              " 'Linear Regression': 2.793982251057454e-05,\n",
              " 'Ridge Regression': 2.7939732208258812e-05}"
            ]
          },
          "execution_count": 273,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "\n",
        "base_models = {\n",
        "    \"Random Forest\": RandomForestRegressor(random_state=42),\n",
        "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
        "    \"Linear Regression\": LinearRegression(),\n",
        "    \"Ridge Regression\": Ridge(random_state=42),\n",
        "}\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "model_scores = {}\n",
        "for name, model in base_models.items():\n",
        "    cv_scores = cross_val_score(model, X_train_processed, y_train, cv=kf, scoring='neg_mean_squared_error')\n",
        "    model_scores[name] = np.mean(np.sqrt(-cv_scores))\n",
        "\n",
        "model_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lo-ffU98n3vz",
        "outputId": "6ab15f23-4dd6-4787-8fee-ec6148466f10"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Random Forest': {'bootstrap': True,\n",
              "  'ccp_alpha': 0.0,\n",
              "  'criterion': 'squared_error',\n",
              "  'max_depth': 10,\n",
              "  'max_features': 1.0,\n",
              "  'max_leaf_nodes': None,\n",
              "  'max_samples': None,\n",
              "  'min_impurity_decrease': 0.0,\n",
              "  'min_samples_leaf': 1,\n",
              "  'min_samples_split': 2,\n",
              "  'min_weight_fraction_leaf': 0.0,\n",
              "  'n_estimators': 150,\n",
              "  'n_jobs': None,\n",
              "  'oob_score': False,\n",
              "  'random_state': 42,\n",
              "  'verbose': 0,\n",
              "  'warm_start': False},\n",
              " 'Gradient Boosting': {'alpha': 0.9,\n",
              "  'ccp_alpha': 0.0,\n",
              "  'criterion': 'friedman_mse',\n",
              "  'init': None,\n",
              "  'learning_rate': 0.2,\n",
              "  'loss': 'squared_error',\n",
              "  'max_depth': 5,\n",
              "  'max_features': None,\n",
              "  'max_leaf_nodes': None,\n",
              "  'min_impurity_decrease': 0.0,\n",
              "  'min_samples_leaf': 1,\n",
              "  'min_samples_split': 2,\n",
              "  'min_weight_fraction_leaf': 0.0,\n",
              "  'n_estimators': 150,\n",
              "  'n_iter_no_change': None,\n",
              "  'random_state': 42,\n",
              "  'subsample': 1.0,\n",
              "  'tol': 0.0001,\n",
              "  'validation_fraction': 0.1,\n",
              "  'verbose': 0,\n",
              "  'warm_start': False},\n",
              " 'Linear Regression': {'copy_X': True,\n",
              "  'fit_intercept': True,\n",
              "  'n_jobs': None,\n",
              "  'positive': False},\n",
              " 'Ridge Regression': {'alpha': 10.0,\n",
              "  'copy_X': True,\n",
              "  'fit_intercept': True,\n",
              "  'max_iter': None,\n",
              "  'positive': False,\n",
              "  'random_state': 42,\n",
              "  'solver': 'auto',\n",
              "  'tol': 0.0001}}"
            ]
          },
          "execution_count": 274,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "param_grid = {\n",
        "    \"Random Forest\": {\n",
        "        'n_estimators': [50, 100, 150],\n",
        "        'max_depth': [None, 5, 10],\n",
        "        'min_samples_split': [2, 4]\n",
        "    },\n",
        "    \"Gradient Boosting\": {\n",
        "        'n_estimators': [50, 100, 150],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7]\n",
        "    },\n",
        "    \"Ridge Regression\": {\n",
        "        'alpha': [0.01, 0.1, 1.0, 10.0]\n",
        "    },\n",
        "}\n",
        "\n",
        "tuned_models = {}\n",
        "for name, model in base_models.items():\n",
        "    if name in param_grid:\n",
        "        grid_search = GridSearchCV(model, param_grid[name], cv=kf, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "        grid_search.fit(X_train_processed, y_train)\n",
        "        tuned_models[name] = grid_search.best_estimator_\n",
        "    else:\n",
        "        tuned_models[name] = model\n",
        "\n",
        "tuned_model_details = {name: model.get_params() for name, model in tuned_models.items()}\n",
        "tuned_model_details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkB6vnTw0RWk",
        "outputId": "8024a37c-1446-447b-9e72-a051b8f87583"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting h2o\n",
            "  Downloading h2o-3.44.0.3.tar.gz (265.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.2/265.2 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from h2o) (2.31.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from h2o) (0.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (2023.11.17)\n",
            "Building wheels for collected packages: h2o\n",
            "  Building wheel for h2o (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for h2o: filename=h2o-3.44.0.3-py2.py3-none-any.whl size=265293968 sha256=8422e4997a2d340643b2109de2a8c2956572b6902d364e2afd8a7bbcab6244b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/9a/1c/2da26f943fd46b57f3c20b54847b936b9152b831dc7447cf71\n",
            "Successfully built h2o\n",
            "Installing collected packages: h2o\n",
            "Successfully installed h2o-3.44.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install h2o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "8pcHAYzaoo4W",
        "outputId": "de0aea56-f4ae-4ce9-808e-4e457f1b6f65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "#h2o-table-6.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-6 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-6 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-6 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-6 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-6 .h2o-table th,\n",
              "#h2o-table-6 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-6 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-6\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption></caption>\n",
              "    <thead></thead>\n",
              "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
              "<td>7 mins 32 secs</td></tr>\n",
              "<tr><td>H2O_cluster_timezone:</td>\n",
              "<td>Etc/UTC</td></tr>\n",
              "<tr><td>H2O_data_parsing_timezone:</td>\n",
              "<td>UTC</td></tr>\n",
              "<tr><td>H2O_cluster_version:</td>\n",
              "<td>3.44.0.3</td></tr>\n",
              "<tr><td>H2O_cluster_version_age:</td>\n",
              "<td>1 month and 2 days</td></tr>\n",
              "<tr><td>H2O_cluster_name:</td>\n",
              "<td>H2O_from_python_unknownUser_pjer56</td></tr>\n",
              "<tr><td>H2O_cluster_total_nodes:</td>\n",
              "<td>1</td></tr>\n",
              "<tr><td>H2O_cluster_free_memory:</td>\n",
              "<td>12.75 Gb</td></tr>\n",
              "<tr><td>H2O_cluster_total_cores:</td>\n",
              "<td>8</td></tr>\n",
              "<tr><td>H2O_cluster_allowed_cores:</td>\n",
              "<td>8</td></tr>\n",
              "<tr><td>H2O_cluster_status:</td>\n",
              "<td>locked, healthy</td></tr>\n",
              "<tr><td>H2O_connection_url:</td>\n",
              "<td>http://localhost:54321</td></tr>\n",
              "<tr><td>H2O_connection_proxy:</td>\n",
              "<td>{\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}</td></tr>\n",
              "<tr><td>H2O_internal_security:</td>\n",
              "<td>False</td></tr>\n",
              "<tr><td>Python_version:</td>\n",
              "<td>3.10.12 final</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n"
            ],
            "text/plain": [
              "--------------------------  -----------------------------------------------------------------------------------------\n",
              "H2O_cluster_uptime:         7 mins 32 secs\n",
              "H2O_cluster_timezone:       Etc/UTC\n",
              "H2O_data_parsing_timezone:  UTC\n",
              "H2O_cluster_version:        3.44.0.3\n",
              "H2O_cluster_version_age:    1 month and 2 days\n",
              "H2O_cluster_name:           H2O_from_python_unknownUser_pjer56\n",
              "H2O_cluster_total_nodes:    1\n",
              "H2O_cluster_free_memory:    12.75 Gb\n",
              "H2O_cluster_total_cores:    8\n",
              "H2O_cluster_allowed_cores:  8\n",
              "H2O_cluster_status:         locked, healthy\n",
              "H2O_connection_url:         http://localhost:54321\n",
              "H2O_connection_proxy:       {\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}\n",
              "H2O_internal_security:      False\n",
              "Python_version:             3.10.12 final\n",
              "--------------------------  -----------------------------------------------------------------------------------------"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "AutoML progress: |███████████████████████████████████████████████████████████████| (done) 100%\n"
          ]
        }
      ],
      "source": [
        "import h2o\n",
        "from h2o.automl import H2OAutoML\n",
        "import pandas as pd\n",
        "\n",
        "h2o.init()\n",
        "\n",
        "meta_X_train = np.column_stack([model.predict(X_train_processed) for model in models_to_train])\n",
        "meta_X_test = np.column_stack([model.predict(X_test_processed) for model in models_to_train])\n",
        "\n",
        "train_df = pd.DataFrame(meta_X_train, columns=[f'feature_{i}' for i in range(meta_X_train.shape[1])])\n",
        "train_df['target'] = y_train.to_numpy()\n",
        "\n",
        "train_h2o = h2o.H2OFrame(train_df)\n",
        "\n",
        "automl = H2OAutoML(max_models=10, seed=42, include_algos=[\"DeepLearning\"])\n",
        "automl.train(x=train_h2o.columns[:-1], y='target', training_frame=train_h2o)\n",
        "\n",
        "best_dl_model = automl.leader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo3tLvn1PUBV",
        "outputId": "8d5f04a4-bccf-4339-d01e-6c009cc7f49e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Das ausgewählte Modell ist vom Typ: deeplearning\n",
            "Modell Hyperparameter:\n",
            "{'model_id': {'default': None, 'actual': {'__meta': {'schema_version': 3, 'schema_name': 'ModelKeyV3', 'schema_type': 'Key<Model>'}, 'name': 'DeepLearning_grid_1_AutoML_1_20240123_10412_model_3', 'type': 'Key<Model>', 'URL': '/3/Models/DeepLearning_grid_1_AutoML_1_20240123_10412_model_3'}, 'input': None}, 'training_frame': {'default': None, 'actual': {'__meta': {'schema_version': 3, 'schema_name': 'FrameKeyV3', 'schema_type': 'Key<Frame>'}, 'name': 'AutoML_1_20240123_10412_training_Key_Frame__upload_976f95d51d4176dcb0f7771e9944a60.hex', 'type': 'Key<Frame>', 'URL': '/3/Frames/AutoML_1_20240123_10412_training_Key_Frame__upload_976f95d51d4176dcb0f7771e9944a60.hex'}, 'input': {'__meta': {'schema_version': 3, 'schema_name': 'FrameKeyV3', 'schema_type': 'Key<Frame>'}, 'name': 'AutoML_1_20240123_10412_training_Key_Frame__upload_976f95d51d4176dcb0f7771e9944a60.hex', 'type': 'Key<Frame>', 'URL': '/3/Frames/AutoML_1_20240123_10412_training_Key_Frame__upload_976f95d51d4176dcb0f7771e9944a60.hex'}}, 'validation_frame': {'default': None, 'actual': None, 'input': None}, 'nfolds': {'default': 0, 'actual': 5, 'input': 5}, 'keep_cross_validation_models': {'default': True, 'actual': False, 'input': False}, 'keep_cross_validation_predictions': {'default': False, 'actual': True, 'input': True}, 'keep_cross_validation_fold_assignment': {'default': False, 'actual': False, 'input': False}, 'fold_assignment': {'default': 'AUTO', 'actual': 'Modulo', 'input': 'Modulo'}, 'fold_column': {'default': None, 'actual': None, 'input': None}, 'response_column': {'default': None, 'actual': {'__meta': {'schema_version': 3, 'schema_name': 'ColSpecifierV3', 'schema_type': 'VecSpecifier'}, 'column_name': 'target', 'is_member_of_frames': None}, 'input': {'__meta': {'schema_version': 3, 'schema_name': 'ColSpecifierV3', 'schema_type': 'VecSpecifier'}, 'column_name': 'target', 'is_member_of_frames': None}}, 'ignored_columns': {'default': None, 'actual': [], 'input': []}, 'ignore_const_cols': {'default': True, 'actual': True, 'input': True}, 'score_each_iteration': {'default': False, 'actual': False, 'input': False}, 'weights_column': {'default': None, 'actual': None, 'input': None}, 'offset_column': {'default': None, 'actual': None, 'input': None}, 'balance_classes': {'default': False, 'actual': False, 'input': False}, 'class_sampling_factors': {'default': None, 'actual': None, 'input': None}, 'max_after_balance_size': {'default': 5.0, 'actual': 5.0, 'input': 5.0}, 'max_confusion_matrix_size': {'default': 20, 'actual': 20, 'input': 20}, 'checkpoint': {'default': None, 'actual': None, 'input': None}, 'pretrained_autoencoder': {'default': None, 'actual': None, 'input': None}, 'overwrite_with_best_model': {'default': True, 'actual': False, 'input': True}, 'use_all_factor_levels': {'default': True, 'actual': True, 'input': True}, 'standardize': {'default': True, 'actual': True, 'input': True}, 'activation': {'default': 'Rectifier', 'actual': 'RectifierWithDropout', 'input': 'RectifierWithDropout'}, 'hidden': {'default': [200, 200], 'actual': [100], 'input': [100]}, 'epochs': {'default': 10.0, 'actual': 8.0, 'input': 10000.0}, 'train_samples_per_iteration': {'default': -2, 'actual': -2, 'input': -2}, 'target_ratio_comm_to_comp': {'default': 0.05, 'actual': 0.05, 'input': 0.05}, 'seed': {'default': -1, 'actual': 44, 'input': 44}, 'adaptive_rate': {'default': True, 'actual': True, 'input': True}, 'rho': {'default': 0.99, 'actual': 0.9, 'input': 0.9}, 'epsilon': {'default': 1e-08, 'actual': 1e-09, 'input': 1e-09}, 'rate': {'default': 0.005, 'actual': 0.005, 'input': 0.005}, 'rate_annealing': {'default': 1e-06, 'actual': 1e-06, 'input': 1e-06}, 'rate_decay': {'default': 1.0, 'actual': 1.0, 'input': 1.0}, 'momentum_start': {'default': 0.0, 'actual': 0.0, 'input': 0.0}, 'momentum_ramp': {'default': 1000000.0, 'actual': 1000000.0, 'input': 1000000.0}, 'momentum_stable': {'default': 0.0, 'actual': 0.0, 'input': 0.0}, 'nesterov_accelerated_gradient': {'default': True, 'actual': True, 'input': True}, 'input_dropout_ratio': {'default': 0.0, 'actual': 0.1, 'input': 0.1}, 'hidden_dropout_ratios': {'default': None, 'actual': [0.0], 'input': [0.0]}, 'l1': {'default': 0.0, 'actual': 0.0, 'input': 0.0}, 'l2': {'default': 0.0, 'actual': 0.0, 'input': 0.0}, 'max_w2': {'default': 3.4028235e+38, 'actual': 3.4028235e+38, 'input': 3.4028235e+38}, 'initial_weight_distribution': {'default': 'UniformAdaptive', 'actual': 'UniformAdaptive', 'input': 'UniformAdaptive'}, 'initial_weight_scale': {'default': 1.0, 'actual': 1.0, 'input': 1.0}, 'initial_weights': {'default': None, 'actual': None, 'input': None}, 'initial_biases': {'default': None, 'actual': None, 'input': None}, 'loss': {'default': 'Automatic', 'actual': 'Automatic', 'input': 'Automatic'}, 'distribution': {'default': 'AUTO', 'actual': 'gaussian', 'input': 'gaussian'}, 'quantile_alpha': {'default': 0.5, 'actual': 0.5, 'input': 0.5}, 'tweedie_power': {'default': 1.5, 'actual': 1.5, 'input': 1.5}, 'huber_alpha': {'default': 0.9, 'actual': 0.9, 'input': 0.9}, 'score_interval': {'default': 5.0, 'actual': 5.0, 'input': 5.0}, 'score_training_samples': {'default': 10000, 'actual': 10000, 'input': 10000}, 'score_validation_samples': {'default': 0, 'actual': 0, 'input': 0}, 'score_duty_cycle': {'default': 0.1, 'actual': 0.1, 'input': 0.1}, 'classification_stop': {'default': 0.0, 'actual': 0.0, 'input': 0.0}, 'regression_stop': {'default': 1e-06, 'actual': 1e-06, 'input': 1e-06}, 'stopping_rounds': {'default': 5, 'actual': 0, 'input': 3}, 'stopping_metric': {'default': 'AUTO', 'actual': 'deviance', 'input': 'deviance'}, 'stopping_tolerance': {'default': 0.0, 'actual': 0.009980059800697489, 'input': 0.009980059800697489}, 'max_runtime_secs': {'default': 0.0, 'actual': 0.0, 'input': 0.0}, 'score_validation_sampling': {'default': 'Uniform', 'actual': 'Uniform', 'input': 'Uniform'}, 'diagnostics': {'default': True, 'actual': True, 'input': True}, 'fast_mode': {'default': True, 'actual': True, 'input': True}, 'force_load_balance': {'default': True, 'actual': True, 'input': True}, 'variable_importances': {'default': True, 'actual': True, 'input': True}, 'replicate_training_data': {'default': True, 'actual': True, 'input': True}, 'single_node_mode': {'default': False, 'actual': False, 'input': False}, 'shuffle_training_data': {'default': False, 'actual': False, 'input': False}, 'missing_values_handling': {'default': 'MeanImputation', 'actual': 'MeanImputation', 'input': 'MeanImputation'}, 'quiet_mode': {'default': False, 'actual': False, 'input': False}, 'autoencoder': {'default': False, 'actual': False, 'input': False}, 'sparse': {'default': False, 'actual': False, 'input': False}, 'col_major': {'default': False, 'actual': False, 'input': False}, 'average_activation': {'default': 0.0, 'actual': 0.0, 'input': 0.0}, 'sparsity_beta': {'default': 0.0, 'actual': 0.0, 'input': 0.0}, 'max_categorical_features': {'default': 2147483647, 'actual': 2147483647, 'input': 2147483647}, 'reproducible': {'default': False, 'actual': False, 'input': False}, 'export_weights_and_biases': {'default': False, 'actual': False, 'input': False}, 'mini_batch_size': {'default': 1, 'actual': 1, 'input': 1}, 'categorical_encoding': {'default': 'AUTO', 'actual': 'OneHotInternal', 'input': 'AUTO'}, 'elastic_averaging': {'default': False, 'actual': False, 'input': False}, 'elastic_averaging_moving_rate': {'default': 0.9, 'actual': 0.9, 'input': 0.9}, 'elastic_averaging_regularization': {'default': 0.001, 'actual': 0.001, 'input': 0.001}, 'export_checkpoints_dir': {'default': None, 'actual': None, 'input': None}, 'auc_type': {'default': 'AUTO', 'actual': 'AUTO', 'input': 'AUTO'}, 'custom_metric_func': {'default': None, 'actual': None, 'input': None}, 'gainslift_bins': {'default': -1, 'actual': -1, 'input': -1}}\n"
          ]
        }
      ],
      "source": [
        "best_model_type = best_dl_model.algo\n",
        "print(f\"Das ausgewählte Modell ist vom Typ: {best_model_type}\")\n",
        "model_params = best_dl_model.params\n",
        "print(f\"Modell Hyperparameter:\\n{model_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xrvmeh37RBK7",
        "outputId": "713fcf10-f375-4345-9c04-3b43577253ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('feature_0', 1.0, 1.0, 0.2789536001069976), ('feature_1', 0.9155796766281128, 0.9155796766281128, 0.25540424698021275), ('feature_2', 0.8856992125511169, 0.8856992125511169, 0.24706898395306695), ('feature_3', 0.7835466861724854, 0.7835466861724854, 0.21857316895972265)]\n"
          ]
        }
      ],
      "source": [
        "feature_importance = best_dl_model.varimp()\n",
        "print(feature_importance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJGSMI0G1Ebb",
        "outputId": "6d38a7f9-0160-4091-fc0f-719bb7da6ee4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
            "Export File progress: |██████████████████████████████████████████████████████████| (done) 100%\n",
            "RMSE: 4.71593160868976e-06\n",
            "R²: 0.9851502425105056\n"
          ]
        }
      ],
      "source": [
        "test_df = pd.DataFrame(meta_X_test, columns=[f'feature_{i}' for i in range(meta_X_test.shape[1])])\n",
        "test_h2o = h2o.H2OFrame(test_df)\n",
        "\n",
        "predictions = best_dl_model.predict(test_h2o)\n",
        "\n",
        "predictions_df = h2o.as_list(predictions)\n",
        "\n",
        "mse = mean_squared_error(y_test, predictions_df)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, predictions_df)\n",
        "\n",
        "print(f\"RMSE: {rmse}\")\n",
        "print(f\"R²: {r2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcSmOBJdz6i2"
      },
      "outputs": [],
      "source": [
        "def format_energy(value_in_kwh):\n",
        "    units = [\n",
        "        (\"TWh\", 1e-9),\n",
        "        (\"GWh\", 1e-6),\n",
        "        (\"MWh\", 1e-3),\n",
        "        (\"kWh\", 1),\n",
        "        (\"Wh\", 1e3),\n",
        "        (\"mWh\", 1e6),\n",
        "        (\"µWh\", 1e9),\n",
        "        (\"nWh\", 1e12),\n",
        "        (\"pWh\", 1e15)\n",
        "    ]\n",
        "\n",
        "    for unit, factor in units:\n",
        "        if value_in_kwh * factor >= 1:\n",
        "            return f\"{value_in_kwh * factor:.2f} {unit}\"\n",
        "\n",
        "    return \"0 µWh\"\n",
        "\n",
        "def predict_and_display_energy_consumption(encoder, num_num_features, num_cat_features, number_of_instances, h2o_model, base_models):\n",
        "    input_data = {\n",
        "        'num_num_features': [num_num_features] * 5,\n",
        "        'num_cat_features': [num_cat_features] * 5,\n",
        "        'number_of_instances': [number_of_instances] * 5,\n",
        "        'model': list(range(5))\n",
        "    }\n",
        "\n",
        "    input_df = pd.DataFrame(input_data)\n",
        "    input_processed = encoder.transform(input_df)\n",
        "\n",
        "    predictions = {}\n",
        "    for model_category in range(5):\n",
        "        # Vorhersagen für Basis-Modelle\n",
        "        for name, model in base_models.items():\n",
        "            prediction = model.predict(input_processed[model_category].reshape(1, -1))[0]\n",
        "            converted_prediction = format_energy(prediction)\n",
        "            model_category_name = list(model_category_mapping.keys())[model_category]\n",
        "            predictions[(model_category_name, name)] = converted_prediction\n",
        "\n",
        "        # Vorhersage für das H2O-Modell\n",
        "        meta_features = np.column_stack([model.predict(input_processed[model_category].reshape(1, -1)) for model in base_models.values()])\n",
        "        meta_features_df = pd.DataFrame(meta_features, columns=[f'feature_{i}' for i in range(meta_features.shape[1])])\n",
        "        meta_features_h2o = h2o.H2OFrame(meta_features_df)\n",
        "        h2o_prediction = h2o_model.predict(meta_features_h2o).as_data_frame().iloc[0, 0]\n",
        "        converted_h2o_prediction = format_energy(h2o_prediction)\n",
        "        predictions[(model_category_name, 'Stacking Ensemble')] = converted_h2o_prediction\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymCSfkHH3jB6",
        "outputId": "bc351e54-270c-4c3d-84e9-24a33a003843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
            "Export File progress: |██████████████████████████████████████████████████████████| (done) 100%\n",
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
            "Export File progress: |██████████████████████████████████████████████████████████| (done) 100%\n",
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
            "Export File progress: |██████████████████████████████████████████████████████████| (done) 100%\n",
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
            "Export File progress: |██████████████████████████████████████████████████████████| (done) 100%\n",
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
            "Export File progress: |██████████████████████████████████████████████████████████| (done) 100%\n",
            "Modell Kategorie 0, Random Forest: 25.32 mWh\n",
            "Modell Kategorie 0, Gradient Boosting: 28.75 mWh\n",
            "Modell Kategorie 0, Linear Regression: 129.22 Wh\n",
            "Modell Kategorie 0, Ridge Regression: 129.32 Wh\n",
            "Modell Kategorie 0, Stacking Ensemble: 16.67 Wh\n",
            "\n",
            "Modell Kategorie 1, Random Forest: 1.89 mWh\n",
            "Modell Kategorie 1, Gradient Boosting: 2.40 mWh\n",
            "Modell Kategorie 1, Linear Regression: 129.22 Wh\n",
            "Modell Kategorie 1, Ridge Regression: 129.32 Wh\n",
            "Modell Kategorie 1, Stacking Ensemble: 16.66 Wh\n",
            "\n",
            "Modell Kategorie 2, Random Forest: 526.11 µWh\n",
            "Modell Kategorie 2, Gradient Boosting: 8.70 mWh\n",
            "Modell Kategorie 2, Linear Regression: 129.22 Wh\n",
            "Modell Kategorie 2, Ridge Regression: 129.32 Wh\n",
            "Modell Kategorie 2, Stacking Ensemble: 16.66 Wh\n",
            "\n",
            "Modell Kategorie 3, Random Forest: 6.87 mWh\n",
            "Modell Kategorie 3, Gradient Boosting: 7.15 mWh\n",
            "Modell Kategorie 3, Linear Regression: 129.24 Wh\n",
            "Modell Kategorie 3, Ridge Regression: 129.33 Wh\n",
            "Modell Kategorie 3, Stacking Ensemble: 16.66 Wh\n",
            "\n",
            "Modell Kategorie 4, Random Forest: 298.74 mWh\n",
            "Modell Kategorie 4, Gradient Boosting: 298.97 mWh\n",
            "Modell Kategorie 4, Linear Regression: 129.29 Wh\n",
            "Modell Kategorie 4, Ridge Regression: 129.38 Wh\n",
            "Modell Kategorie 4, Stacking Ensemble: 16.79 Wh\n"
          ]
        }
      ],
      "source": [
        "final_predictions = predict_and_display_energy_consumption(\n",
        "    encoder=preprocessor,\n",
        "    num_num_features=50000,\n",
        "    num_cat_features=13000,\n",
        "    number_of_instances=1000000,\n",
        "    h2o_model=best_dl_model,\n",
        "    base_models=tuned_models\n",
        ")\n",
        "\n",
        "output_lines = [f\"Modell Kategorie {model_category}, {model_name}: {prediction}\" for (model_category, model_name), prediction in final_predictions.items()]\n",
        "output_paragraphs = ['\\n'.join(output_lines[i:i+5]) for i in range(0, len(output_lines), 5)]\n",
        "print('\\n\\n'.join(output_paragraphs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "zwyTXBPoTsGO",
        "outputId": "b190b976-021c-4294-8102-67d5eba36998"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "#h2o-table-8.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-8 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-8 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-8 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-8 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-8 .h2o-table th,\n",
              "#h2o-table-8 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-8 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-8\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Status of Neuron Layers: predicting target, regression, gaussian distribution, Quadratic loss, 601 weights/biases, 11.3 KB, 8,074 training samples, mini-batch size 1</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>layer</th>\n",
              "<th>units</th>\n",
              "<th>type</th>\n",
              "<th>dropout</th>\n",
              "<th>l1</th>\n",
              "<th>l2</th>\n",
              "<th>mean_rate</th>\n",
              "<th>rate_rms</th>\n",
              "<th>momentum</th>\n",
              "<th>mean_weight</th>\n",
              "<th>weight_rms</th>\n",
              "<th>mean_bias</th>\n",
              "<th>bias_rms</th></tr></thead>\n",
              "    <tbody><tr><td></td>\n",
              "<td>1</td>\n",
              "<td>4</td>\n",
              "<td>Input</td>\n",
              "<td>10.0</td>\n",
              "<td></td>\n",
              "<td></td>\n",
              "<td></td>\n",
              "<td></td>\n",
              "<td></td>\n",
              "<td></td>\n",
              "<td></td>\n",
              "<td></td>\n",
              "<td></td></tr>\n",
              "<tr><td></td>\n",
              "<td>2</td>\n",
              "<td>100</td>\n",
              "<td>RectifierDropout</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0033349</td>\n",
              "<td>0.0283144</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0101934</td>\n",
              "<td>0.1313232</td>\n",
              "<td>0.5018093</td>\n",
              "<td>0.0070797</td></tr>\n",
              "<tr><td></td>\n",
              "<td>3</td>\n",
              "<td>1</td>\n",
              "<td>Linear</td>\n",
              "<td></td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0001045</td>\n",
              "<td>0.0001039</td>\n",
              "<td>0.0</td>\n",
              "<td>-0.0002658</td>\n",
              "<td>0.1410509</td>\n",
              "<td>0.0205677</td>\n",
              "<td>0.0000000</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n"
            ],
            "text/plain": [
              "Status of Neuron Layers: predicting target, regression, gaussian distribution, Quadratic loss, 601 weights/biases, 11.3 KB, 8,074 training samples, mini-batch size 1\n",
              "    layer    units    type              dropout    l1    l2    mean_rate              rate_rms                momentum    mean_weight              weight_rms           mean_bias             bias_rms\n",
              "--  -------  -------  ----------------  ---------  ----  ----  ---------------------  ----------------------  ----------  -----------------------  -------------------  --------------------  -----------------------\n",
              "    1        4        Input             10.0\n",
              "    2        100      RectifierDropout  0.0        0.0   0.0   0.0033348902535544765  0.028314441442489624    0.0         0.010193409974817769     0.1313232183456421   0.5018092855514387    0.007079664617776871\n",
              "    3        1        Linear                       0.0   0.0   0.0001045001981765381  0.00010391898103989661  0.0         -0.00026576107833534477  0.14105093479156494  0.020567710163967502  1.0971281125650402e-154"
            ]
          },
          "execution_count": 466,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model=best_dl_model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "4I2TKlTIWrnH",
        "outputId": "30ebe290-e15a-4cda-f7be-79e3e19d2471"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "deepfeatures progress: |█████████████████████████████████████████████████████████| (done) 100%\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table class='dataframe'>\n",
              "<thead>\n",
              "<tr><th style=\"text-align: right;\">  DF.L1.C1</th><th style=\"text-align: right;\">  DF.L1.C2</th><th style=\"text-align: right;\">  DF.L1.C3</th><th style=\"text-align: right;\">  DF.L1.C4</th><th style=\"text-align: right;\">  DF.L1.C5</th><th style=\"text-align: right;\">  DF.L1.C6</th><th style=\"text-align: right;\">  DF.L1.C7</th><th style=\"text-align: right;\">  DF.L1.C8</th><th style=\"text-align: right;\">  DF.L1.C9</th><th style=\"text-align: right;\">  DF.L1.C10</th><th style=\"text-align: right;\">  DF.L1.C11</th><th style=\"text-align: right;\">  DF.L1.C12</th><th style=\"text-align: right;\">  DF.L1.C13</th><th style=\"text-align: right;\">  DF.L1.C14</th><th style=\"text-align: right;\">  DF.L1.C15</th><th style=\"text-align: right;\">  DF.L1.C16</th><th style=\"text-align: right;\">  DF.L1.C17</th><th style=\"text-align: right;\">  DF.L1.C18</th><th style=\"text-align: right;\">  DF.L1.C19</th><th style=\"text-align: right;\">  DF.L1.C20</th><th style=\"text-align: right;\">  DF.L1.C21</th><th style=\"text-align: right;\">  DF.L1.C22</th><th style=\"text-align: right;\">  DF.L1.C23</th><th style=\"text-align: right;\">  DF.L1.C24</th><th style=\"text-align: right;\">  DF.L1.C25</th><th style=\"text-align: right;\">  DF.L1.C26</th><th style=\"text-align: right;\">  DF.L1.C27</th><th style=\"text-align: right;\">  DF.L1.C28</th><th style=\"text-align: right;\">  DF.L1.C29</th><th style=\"text-align: right;\">  DF.L1.C30</th><th style=\"text-align: right;\">  DF.L1.C31</th><th style=\"text-align: right;\">  DF.L1.C32</th><th style=\"text-align: right;\">  DF.L1.C33</th><th style=\"text-align: right;\">  DF.L1.C34</th><th style=\"text-align: right;\">  DF.L1.C35</th><th style=\"text-align: right;\">  DF.L1.C36</th><th style=\"text-align: right;\">  DF.L1.C37</th><th style=\"text-align: right;\">  DF.L1.C38</th><th style=\"text-align: right;\">  DF.L1.C39</th><th style=\"text-align: right;\">  DF.L1.C40</th><th style=\"text-align: right;\">  DF.L1.C41</th><th style=\"text-align: right;\">  DF.L1.C42</th><th style=\"text-align: right;\">  DF.L1.C43</th><th style=\"text-align: right;\">  DF.L1.C44</th><th style=\"text-align: right;\">  DF.L1.C45</th><th style=\"text-align: right;\">  DF.L1.C46</th><th style=\"text-align: right;\">  DF.L1.C47</th><th style=\"text-align: right;\">  DF.L1.C48</th><th style=\"text-align: right;\">  DF.L1.C49</th><th style=\"text-align: right;\">  DF.L1.C50</th><th style=\"text-align: right;\">  DF.L1.C51</th><th style=\"text-align: right;\">  DF.L1.C52</th><th style=\"text-align: right;\">  DF.L1.C53</th><th style=\"text-align: right;\">  DF.L1.C54</th><th style=\"text-align: right;\">  DF.L1.C55</th><th style=\"text-align: right;\">  DF.L1.C56</th><th style=\"text-align: right;\">  DF.L1.C57</th><th style=\"text-align: right;\">  DF.L1.C58</th><th style=\"text-align: right;\">  DF.L1.C59</th><th style=\"text-align: right;\">  DF.L1.C60</th><th style=\"text-align: right;\">  DF.L1.C61</th><th style=\"text-align: right;\">  DF.L1.C62</th><th style=\"text-align: right;\">  DF.L1.C63</th><th style=\"text-align: right;\">  DF.L1.C64</th><th style=\"text-align: right;\">  DF.L1.C65</th><th style=\"text-align: right;\">  DF.L1.C66</th><th style=\"text-align: right;\">  DF.L1.C67</th><th style=\"text-align: right;\">  DF.L1.C68</th><th style=\"text-align: right;\">  DF.L1.C69</th><th style=\"text-align: right;\">  DF.L1.C70</th><th style=\"text-align: right;\">  DF.L1.C71</th><th style=\"text-align: right;\">  DF.L1.C72</th><th style=\"text-align: right;\">  DF.L1.C73</th><th style=\"text-align: right;\">  DF.L1.C74</th><th style=\"text-align: right;\">  DF.L1.C75</th><th style=\"text-align: right;\">  DF.L1.C76</th><th style=\"text-align: right;\">  DF.L1.C77</th><th style=\"text-align: right;\">  DF.L1.C78</th><th style=\"text-align: right;\">  DF.L1.C79</th><th style=\"text-align: right;\">  DF.L1.C80</th><th style=\"text-align: right;\">  DF.L1.C81</th><th style=\"text-align: right;\">  DF.L1.C82</th><th style=\"text-align: right;\">  DF.L1.C83</th><th style=\"text-align: right;\">  DF.L1.C84</th><th style=\"text-align: right;\">  DF.L1.C85</th><th style=\"text-align: right;\">  DF.L1.C86</th><th style=\"text-align: right;\">  DF.L1.C87</th><th style=\"text-align: right;\">  DF.L1.C88</th><th style=\"text-align: right;\">  DF.L1.C89</th><th style=\"text-align: right;\">  DF.L1.C90</th><th style=\"text-align: right;\">  DF.L1.C91</th><th style=\"text-align: right;\">  DF.L1.C92</th><th style=\"text-align: right;\">  DF.L1.C93</th><th style=\"text-align: right;\">  DF.L1.C94</th><th style=\"text-align: right;\">  DF.L1.C95</th><th style=\"text-align: right;\">  DF.L1.C96</th><th style=\"text-align: right;\">  DF.L1.C97</th><th style=\"text-align: right;\">  DF.L1.C98</th><th style=\"text-align: right;\">  DF.L1.C99</th><th style=\"text-align: right;\">  DF.L1.C100</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td style=\"text-align: right;\">  0.476866</td><td style=\"text-align: right;\">  0.206894</td><td style=\"text-align: right;\">  0.450783</td><td style=\"text-align: right;\">  0.649465</td><td style=\"text-align: right;\">  0.539621</td><td style=\"text-align: right;\"> 0.179717 </td><td style=\"text-align: right;\">  0.40628 </td><td style=\"text-align: right;\">  0.464371</td><td style=\"text-align: right;\">  0.591009</td><td style=\"text-align: right;\">   0.345847</td><td style=\"text-align: right;\">   0.844423</td><td style=\"text-align: right;\">   0.632539</td><td style=\"text-align: right;\">   0.779635</td><td style=\"text-align: right;\">   0.491806</td><td style=\"text-align: right;\">   0.427308</td><td style=\"text-align: right;\">   0.480988</td><td style=\"text-align: right;\">   0.293482</td><td style=\"text-align: right;\">   0.596271</td><td style=\"text-align: right;\">   0.413662</td><td style=\"text-align: right;\">   0.390864</td><td style=\"text-align: right;\">   0.749717</td><td style=\"text-align: right;\">   0.445975</td><td style=\"text-align: right;\">   0.309583</td><td style=\"text-align: right;\">   0.450759</td><td style=\"text-align: right;\">   0.493889</td><td style=\"text-align: right;\">   0.688875</td><td style=\"text-align: right;\">   0.517312</td><td style=\"text-align: right;\">   0.719213</td><td style=\"text-align: right;\">  0.20324  </td><td style=\"text-align: right;\">   0.694822</td><td style=\"text-align: right;\">   0.667202</td><td style=\"text-align: right;\">   0.470412</td><td style=\"text-align: right;\">   0.401534</td><td style=\"text-align: right;\">   0.677126</td><td style=\"text-align: right;\">   0.591005</td><td style=\"text-align: right;\">   0.213191</td><td style=\"text-align: right;\">   0.365171</td><td style=\"text-align: right;\">   0.525412</td><td style=\"text-align: right;\">   0.64191 </td><td style=\"text-align: right;\">   0.62637 </td><td style=\"text-align: right;\">   0.50403 </td><td style=\"text-align: right;\">   0.377655</td><td style=\"text-align: right;\">   0.584738</td><td style=\"text-align: right;\">   0.568365</td><td style=\"text-align: right;\">   0.538596</td><td style=\"text-align: right;\">   0.62458 </td><td style=\"text-align: right;\">   0.550305</td><td style=\"text-align: right;\">  0.156256 </td><td style=\"text-align: right;\">   0.324684</td><td style=\"text-align: right;\">   0.256291</td><td style=\"text-align: right;\">   0.599857</td><td style=\"text-align: right;\">   0.561941</td><td style=\"text-align: right;\">   0.484416</td><td style=\"text-align: right;\">   0.231673</td><td style=\"text-align: right;\">   0.417029</td><td style=\"text-align: right;\">   0.632142</td><td style=\"text-align: right;\">   0.422291</td><td style=\"text-align: right;\">   0.698298</td><td style=\"text-align: right;\">   0.296374</td><td style=\"text-align: right;\">   0.309958</td><td style=\"text-align: right;\">  0.706764 </td><td style=\"text-align: right;\">   0.724398</td><td style=\"text-align: right;\">   0.72156 </td><td style=\"text-align: right;\">   0.43064 </td><td style=\"text-align: right;\"> 0.124262  </td><td style=\"text-align: right;\">   0.477825</td><td style=\"text-align: right;\">   0.639924</td><td style=\"text-align: right;\">   0.561492</td><td style=\"text-align: right;\">   0.568198</td><td style=\"text-align: right;\">   0.546074</td><td style=\"text-align: right;\">   0.550986</td><td style=\"text-align: right;\">   0.311592</td><td style=\"text-align: right;\">   0.447869</td><td style=\"text-align: right;\">   0.544299</td><td style=\"text-align: right;\">   0.629252</td><td style=\"text-align: right;\">   0.557639</td><td style=\"text-align: right;\">   0.699405</td><td style=\"text-align: right;\">   0.484068</td><td style=\"text-align: right;\">   0.57992 </td><td style=\"text-align: right;\">   0.263858</td><td style=\"text-align: right;\">   0.392426</td><td style=\"text-align: right;\">   0.829131</td><td style=\"text-align: right;\">   0.279419</td><td style=\"text-align: right;\">   0.611305</td><td style=\"text-align: right;\">   0.500395</td><td style=\"text-align: right;\">   0.501034</td><td style=\"text-align: right;\">   0.516754</td><td style=\"text-align: right;\">   0.446399</td><td style=\"text-align: right;\">   0.302181</td><td style=\"text-align: right;\">   0.408349</td><td style=\"text-align: right;\">   0.2401  </td><td style=\"text-align: right;\">  0.173322 </td><td style=\"text-align: right;\">   0.360124</td><td style=\"text-align: right;\">   0.487374</td><td style=\"text-align: right;\">   0.55881 </td><td style=\"text-align: right;\">   0.299605</td><td style=\"text-align: right;\">   0.366409</td><td style=\"text-align: right;\">   0.569901</td><td style=\"text-align: right;\">   0.550494</td><td style=\"text-align: right;\">    0.424736</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.477317</td><td style=\"text-align: right;\">  0.280099</td><td style=\"text-align: right;\">  0.441929</td><td style=\"text-align: right;\">  0.617288</td><td style=\"text-align: right;\">  0.519714</td><td style=\"text-align: right;\"> 0.266042 </td><td style=\"text-align: right;\">  0.41942 </td><td style=\"text-align: right;\">  0.450049</td><td style=\"text-align: right;\">  0.552145</td><td style=\"text-align: right;\">   0.385475</td><td style=\"text-align: right;\">   0.744338</td><td style=\"text-align: right;\">   0.612708</td><td style=\"text-align: right;\">   0.696234</td><td style=\"text-align: right;\">   0.492979</td><td style=\"text-align: right;\">   0.441851</td><td style=\"text-align: right;\">   0.507381</td><td style=\"text-align: right;\">   0.340426</td><td style=\"text-align: right;\">   0.573147</td><td style=\"text-align: right;\">   0.430916</td><td style=\"text-align: right;\">   0.415872</td><td style=\"text-align: right;\">   0.670374</td><td style=\"text-align: right;\">   0.465169</td><td style=\"text-align: right;\">   0.359586</td><td style=\"text-align: right;\">   0.4616  </td><td style=\"text-align: right;\">   0.506406</td><td style=\"text-align: right;\">   0.6452  </td><td style=\"text-align: right;\">   0.516537</td><td style=\"text-align: right;\">   0.657574</td><td style=\"text-align: right;\">  0.287249 </td><td style=\"text-align: right;\">   0.626406</td><td style=\"text-align: right;\">   0.603958</td><td style=\"text-align: right;\">   0.466854</td><td style=\"text-align: right;\">   0.432003</td><td style=\"text-align: right;\">   0.625039</td><td style=\"text-align: right;\">   0.565623</td><td style=\"text-align: right;\">   0.287412</td><td style=\"text-align: right;\">   0.399658</td><td style=\"text-align: right;\">   0.541953</td><td style=\"text-align: right;\">   0.608491</td><td style=\"text-align: right;\">   0.604101</td><td style=\"text-align: right;\">   0.491838</td><td style=\"text-align: right;\">   0.402203</td><td style=\"text-align: right;\">   0.556971</td><td style=\"text-align: right;\">   0.555472</td><td style=\"text-align: right;\">   0.532367</td><td style=\"text-align: right;\">   0.559124</td><td style=\"text-align: right;\">   0.552426</td><td style=\"text-align: right;\">  0.249121 </td><td style=\"text-align: right;\">   0.381221</td><td style=\"text-align: right;\">   0.326268</td><td style=\"text-align: right;\">   0.568427</td><td style=\"text-align: right;\">   0.552486</td><td style=\"text-align: right;\">   0.48559 </td><td style=\"text-align: right;\">   0.2997  </td><td style=\"text-align: right;\">   0.417678</td><td style=\"text-align: right;\">   0.607292</td><td style=\"text-align: right;\">   0.443875</td><td style=\"text-align: right;\">   0.666979</td><td style=\"text-align: right;\">   0.353408</td><td style=\"text-align: right;\">   0.366634</td><td style=\"text-align: right;\">  0.644096 </td><td style=\"text-align: right;\">   0.644065</td><td style=\"text-align: right;\">   0.647085</td><td style=\"text-align: right;\">   0.431369</td><td style=\"text-align: right;\"> 0.226893  </td><td style=\"text-align: right;\">   0.483456</td><td style=\"text-align: right;\">   0.620899</td><td style=\"text-align: right;\">   0.539412</td><td style=\"text-align: right;\">   0.560657</td><td style=\"text-align: right;\">   0.518868</td><td style=\"text-align: right;\">   0.535126</td><td style=\"text-align: right;\">   0.368285</td><td style=\"text-align: right;\">   0.480039</td><td style=\"text-align: right;\">   0.550057</td><td style=\"text-align: right;\">   0.591539</td><td style=\"text-align: right;\">   0.52698 </td><td style=\"text-align: right;\">   0.65535 </td><td style=\"text-align: right;\">   0.495579</td><td style=\"text-align: right;\">   0.551691</td><td style=\"text-align: right;\">   0.324845</td><td style=\"text-align: right;\">   0.422938</td><td style=\"text-align: right;\">   0.723827</td><td style=\"text-align: right;\">   0.348193</td><td style=\"text-align: right;\">   0.596786</td><td style=\"text-align: right;\">   0.488226</td><td style=\"text-align: right;\">   0.49475 </td><td style=\"text-align: right;\">   0.51174 </td><td style=\"text-align: right;\">   0.442767</td><td style=\"text-align: right;\">   0.351048</td><td style=\"text-align: right;\">   0.422419</td><td style=\"text-align: right;\">   0.309533</td><td style=\"text-align: right;\">  0.254511 </td><td style=\"text-align: right;\">   0.407451</td><td style=\"text-align: right;\">   0.503698</td><td style=\"text-align: right;\">   0.537192</td><td style=\"text-align: right;\">   0.355856</td><td style=\"text-align: right;\">   0.385372</td><td style=\"text-align: right;\">   0.545199</td><td style=\"text-align: right;\">   0.542581</td><td style=\"text-align: right;\">    0.431506</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.430792</td><td style=\"text-align: right;\">  0.844291</td><td style=\"text-align: right;\">  0.21881 </td><td style=\"text-align: right;\">  0.407217</td><td style=\"text-align: right;\">  0.285391</td><td style=\"text-align: right;\"> 0.97523  </td><td style=\"text-align: right;\">  0.447026</td><td style=\"text-align: right;\">  0.171927</td><td style=\"text-align: right;\">  0.138119</td><td style=\"text-align: right;\">   0.701054</td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">   0.556361</td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">   0.484468</td><td style=\"text-align: right;\">   0.540933</td><td style=\"text-align: right;\">   0.848335</td><td style=\"text-align: right;\">   0.6734  </td><td style=\"text-align: right;\">   0.382539</td><td style=\"text-align: right;\">   0.531024</td><td style=\"text-align: right;\">   0.602757</td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">   0.664165</td><td style=\"text-align: right;\">   0.756833</td><td style=\"text-align: right;\">   0.546111</td><td style=\"text-align: right;\">   0.673279</td><td style=\"text-align: right;\">   0.303067</td><td style=\"text-align: right;\">   0.540185</td><td style=\"text-align: right;\">   0.10241 </td><td style=\"text-align: right;\">  1.01847  </td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">   0.374151</td><td style=\"text-align: right;\">   0.697945</td><td style=\"text-align: right;\">   0.178785</td><td style=\"text-align: right;\">   0.3409  </td><td style=\"text-align: right;\">   0.879865</td><td style=\"text-align: right;\">   0.67191 </td><td style=\"text-align: right;\">   0.803213</td><td style=\"text-align: right;\">   0.34038 </td><td style=\"text-align: right;\">   0.510869</td><td style=\"text-align: right;\">   0.313586</td><td style=\"text-align: right;\">   0.551029</td><td style=\"text-align: right;\">   0.303553</td><td style=\"text-align: right;\">   0.465938</td><td style=\"text-align: right;\">   0.501075</td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">   0.65273 </td><td style=\"text-align: right;\">  1.01729  </td><td style=\"text-align: right;\">   0.917633</td><td style=\"text-align: right;\">   0.940552</td><td style=\"text-align: right;\">   0.272944</td><td style=\"text-align: right;\">   0.505037</td><td style=\"text-align: right;\">   0.480383</td><td style=\"text-align: right;\">   0.826736</td><td style=\"text-align: right;\">   0.278753</td><td style=\"text-align: right;\">   0.486329</td><td style=\"text-align: right;\">   0.620161</td><td style=\"text-align: right;\">   0.50022 </td><td style=\"text-align: right;\">   0.83495 </td><td style=\"text-align: right;\">   0.87961 </td><td style=\"text-align: right;\">  0.0640937</td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">   0.319953</td><td style=\"text-align: right;\"> 1.08314   </td><td style=\"text-align: right;\">   0.540833</td><td style=\"text-align: right;\">   0.557628</td><td style=\"text-align: right;\">   0.327804</td><td style=\"text-align: right;\">   0.569481</td><td style=\"text-align: right;\">   0.190697</td><td style=\"text-align: right;\">   0.388302</td><td style=\"text-align: right;\">   0.866509</td><td style=\"text-align: right;\">   0.851718</td><td style=\"text-align: right;\">   0.693611</td><td style=\"text-align: right;\">   0.248035</td><td style=\"text-align: right;\">   0.173231</td><td style=\"text-align: right;\">   0.323887</td><td style=\"text-align: right;\">   0.641123</td><td style=\"text-align: right;\">   0.262343</td><td style=\"text-align: right;\">   0.825806</td><td style=\"text-align: right;\">   0.689262</td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">   0.968292</td><td style=\"text-align: right;\">   0.554018</td><td style=\"text-align: right;\">   0.323948</td><td style=\"text-align: right;\">   0.407817</td><td style=\"text-align: right;\">   0.466666</td><td style=\"text-align: right;\">   0.297718</td><td style=\"text-align: right;\">   0.741396</td><td style=\"text-align: right;\">   0.460532</td><td style=\"text-align: right;\">   0.892748</td><td style=\"text-align: right;\">  0.898098 </td><td style=\"text-align: right;\">   0.850295</td><td style=\"text-align: right;\">   0.713407</td><td style=\"text-align: right;\">   0.310118</td><td style=\"text-align: right;\">   0.829867</td><td style=\"text-align: right;\">   0.432174</td><td style=\"text-align: right;\">   0.302838</td><td style=\"text-align: right;\">   0.500999</td><td style=\"text-align: right;\">    0.411288</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.475253</td><td style=\"text-align: right;\">  0.247348</td><td style=\"text-align: right;\">  0.44025 </td><td style=\"text-align: right;\">  0.634063</td><td style=\"text-align: right;\">  0.525619</td><td style=\"text-align: right;\"> 0.229905 </td><td style=\"text-align: right;\">  0.41182 </td><td style=\"text-align: right;\">  0.452342</td><td style=\"text-align: right;\">  0.566636</td><td style=\"text-align: right;\">   0.369103</td><td style=\"text-align: right;\">   0.785181</td><td style=\"text-align: right;\">   0.624456</td><td style=\"text-align: right;\">   0.727473</td><td style=\"text-align: right;\">   0.489785</td><td style=\"text-align: right;\">   0.435017</td><td style=\"text-align: right;\">   0.499991</td><td style=\"text-align: right;\">   0.318573</td><td style=\"text-align: right;\">   0.586294</td><td style=\"text-align: right;\">   0.42007 </td><td style=\"text-align: right;\">   0.407143</td><td style=\"text-align: right;\">   0.698772</td><td style=\"text-align: right;\">   0.461484</td><td style=\"text-align: right;\">   0.336032</td><td style=\"text-align: right;\">   0.454937</td><td style=\"text-align: right;\">   0.504409</td><td style=\"text-align: right;\">   0.66553 </td><td style=\"text-align: right;\">   0.521613</td><td style=\"text-align: right;\">   0.682302</td><td style=\"text-align: right;\">  0.253943 </td><td style=\"text-align: right;\">   0.650813</td><td style=\"text-align: right;\">   0.627196</td><td style=\"text-align: right;\">   0.466635</td><td style=\"text-align: right;\">   0.417365</td><td style=\"text-align: right;\">   0.645862</td><td style=\"text-align: right;\">   0.574131</td><td style=\"text-align: right;\">   0.256574</td><td style=\"text-align: right;\">   0.386629</td><td style=\"text-align: right;\">   0.537966</td><td style=\"text-align: right;\">   0.620042</td><td style=\"text-align: right;\">   0.615127</td><td style=\"text-align: right;\">   0.496441</td><td style=\"text-align: right;\">   0.388659</td><td style=\"text-align: right;\">   0.568402</td><td style=\"text-align: right;\">   0.558507</td><td style=\"text-align: right;\">   0.532509</td><td style=\"text-align: right;\">   0.58059 </td><td style=\"text-align: right;\">   0.5558  </td><td style=\"text-align: right;\">  0.209727 </td><td style=\"text-align: right;\">   0.357494</td><td style=\"text-align: right;\">   0.299882</td><td style=\"text-align: right;\">   0.578258</td><td style=\"text-align: right;\">   0.554717</td><td style=\"text-align: right;\">   0.482209</td><td style=\"text-align: right;\">   0.271287</td><td style=\"text-align: right;\">   0.413427</td><td style=\"text-align: right;\">   0.621137</td><td style=\"text-align: right;\">   0.43666 </td><td style=\"text-align: right;\">   0.683072</td><td style=\"text-align: right;\">   0.326602</td><td style=\"text-align: right;\">   0.340926</td><td style=\"text-align: right;\">  0.668888 </td><td style=\"text-align: right;\">   0.671938</td><td style=\"text-align: right;\">   0.676193</td><td style=\"text-align: right;\">   0.426081</td><td style=\"text-align: right;\"> 0.186373  </td><td style=\"text-align: right;\">   0.483958</td><td style=\"text-align: right;\">   0.632354</td><td style=\"text-align: right;\">   0.548167</td><td style=\"text-align: right;\">   0.564677</td><td style=\"text-align: right;\">   0.529386</td><td style=\"text-align: right;\">   0.543687</td><td style=\"text-align: right;\">   0.348359</td><td style=\"text-align: right;\">   0.47036 </td><td style=\"text-align: right;\">   0.552165</td><td style=\"text-align: right;\">   0.607978</td><td style=\"text-align: right;\">   0.534012</td><td style=\"text-align: right;\">   0.67612 </td><td style=\"text-align: right;\">   0.490463</td><td style=\"text-align: right;\">   0.562124</td><td style=\"text-align: right;\">   0.297452</td><td style=\"text-align: right;\">   0.411947</td><td style=\"text-align: right;\">   0.768756</td><td style=\"text-align: right;\">   0.321116</td><td style=\"text-align: right;\">   0.603064</td><td style=\"text-align: right;\">   0.492511</td><td style=\"text-align: right;\">   0.498419</td><td style=\"text-align: right;\">   0.513818</td><td style=\"text-align: right;\">   0.439645</td><td style=\"text-align: right;\">   0.329483</td><td style=\"text-align: right;\">   0.414258</td><td style=\"text-align: right;\">   0.278461</td><td style=\"text-align: right;\">  0.218764 </td><td style=\"text-align: right;\">   0.387821</td><td style=\"text-align: right;\">   0.501183</td><td style=\"text-align: right;\">   0.546238</td><td style=\"text-align: right;\">   0.332626</td><td style=\"text-align: right;\">   0.372885</td><td style=\"text-align: right;\">   0.550831</td><td style=\"text-align: right;\">   0.542662</td><td style=\"text-align: right;\">    0.425819</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.476208</td><td style=\"text-align: right;\">  0.207308</td><td style=\"text-align: right;\">  0.448446</td><td style=\"text-align: right;\">  0.6504  </td><td style=\"text-align: right;\">  0.538316</td><td style=\"text-align: right;\"> 0.181371 </td><td style=\"text-align: right;\">  0.405701</td><td style=\"text-align: right;\">  0.462882</td><td style=\"text-align: right;\">  0.589632</td><td style=\"text-align: right;\">   0.346635</td><td style=\"text-align: right;\">   0.841904</td><td style=\"text-align: right;\">   0.633728</td><td style=\"text-align: right;\">   0.776401</td><td style=\"text-align: right;\">   0.490537</td><td style=\"text-align: right;\">   0.427273</td><td style=\"text-align: right;\">   0.482895</td><td style=\"text-align: right;\">   0.293303</td><td style=\"text-align: right;\">   0.597817</td><td style=\"text-align: right;\">   0.412167</td><td style=\"text-align: right;\">   0.392269</td><td style=\"text-align: right;\">   0.745979</td><td style=\"text-align: right;\">   0.448354</td><td style=\"text-align: right;\">   0.309235</td><td style=\"text-align: right;\">   0.449832</td><td style=\"text-align: right;\">   0.495577</td><td style=\"text-align: right;\">   0.689044</td><td style=\"text-align: right;\">   0.519612</td><td style=\"text-align: right;\">   0.717642</td><td style=\"text-align: right;\">  0.205679 </td><td style=\"text-align: right;\">   0.691676</td><td style=\"text-align: right;\">   0.664792</td><td style=\"text-align: right;\">   0.469723</td><td style=\"text-align: right;\">   0.401132</td><td style=\"text-align: right;\">   0.675692</td><td style=\"text-align: right;\">   0.589608</td><td style=\"text-align: right;\">   0.214738</td><td style=\"text-align: right;\">   0.366501</td><td style=\"text-align: right;\">   0.526852</td><td style=\"text-align: right;\">   0.640001</td><td style=\"text-align: right;\">   0.626353</td><td style=\"text-align: right;\">   0.503886</td><td style=\"text-align: right;\">   0.376494</td><td style=\"text-align: right;\">   0.584204</td><td style=\"text-align: right;\">   0.566908</td><td style=\"text-align: right;\">   0.537131</td><td style=\"text-align: right;\">   0.620991</td><td style=\"text-align: right;\">   0.552314</td><td style=\"text-align: right;\">  0.157681 </td><td style=\"text-align: right;\">   0.325394</td><td style=\"text-align: right;\">   0.259061</td><td style=\"text-align: right;\">   0.597649</td><td style=\"text-align: right;\">   0.560697</td><td style=\"text-align: right;\">   0.48303 </td><td style=\"text-align: right;\">   0.233245</td><td style=\"text-align: right;\">   0.415582</td><td style=\"text-align: right;\">   0.633058</td><td style=\"text-align: right;\">   0.423659</td><td style=\"text-align: right;\">   0.699009</td><td style=\"text-align: right;\">   0.29579 </td><td style=\"text-align: right;\">   0.309788</td><td style=\"text-align: right;\">  0.705081 </td><td style=\"text-align: right;\">   0.720301</td><td style=\"text-align: right;\">   0.719469</td><td style=\"text-align: right;\">   0.428452</td><td style=\"text-align: right;\"> 0.127445  </td><td style=\"text-align: right;\">   0.479283</td><td style=\"text-align: right;\">   0.640903</td><td style=\"text-align: right;\">   0.560981</td><td style=\"text-align: right;\">   0.568193</td><td style=\"text-align: right;\">   0.5456  </td><td style=\"text-align: right;\">   0.551936</td><td style=\"text-align: right;\">   0.314632</td><td style=\"text-align: right;\">   0.450124</td><td style=\"text-align: right;\">   0.5464  </td><td style=\"text-align: right;\">   0.629126</td><td style=\"text-align: right;\">   0.554424</td><td style=\"text-align: right;\">   0.699704</td><td style=\"text-align: right;\">   0.483842</td><td style=\"text-align: right;\">   0.579009</td><td style=\"text-align: right;\">   0.26395 </td><td style=\"text-align: right;\">   0.394005</td><td style=\"text-align: right;\">   0.827994</td><td style=\"text-align: right;\">   0.281434</td><td style=\"text-align: right;\">   0.610858</td><td style=\"text-align: right;\">   0.499927</td><td style=\"text-align: right;\">   0.501753</td><td style=\"text-align: right;\">   0.516525</td><td style=\"text-align: right;\">   0.44422 </td><td style=\"text-align: right;\">   0.3025  </td><td style=\"text-align: right;\">   0.407853</td><td style=\"text-align: right;\">   0.240263</td><td style=\"text-align: right;\">  0.173972 </td><td style=\"text-align: right;\">   0.360941</td><td style=\"text-align: right;\">   0.489643</td><td style=\"text-align: right;\">   0.558706</td><td style=\"text-align: right;\">   0.300787</td><td style=\"text-align: right;\">   0.364753</td><td style=\"text-align: right;\">   0.567018</td><td style=\"text-align: right;\">   0.548561</td><td style=\"text-align: right;\">    0.423698</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.486466</td><td style=\"text-align: right;\">  0.129757</td><td style=\"text-align: right;\">  0.492849</td><td style=\"text-align: right;\">  0.675551</td><td style=\"text-align: right;\">  0.577524</td><td style=\"text-align: right;\"> 0.0795401</td><td style=\"text-align: right;\">  0.408679</td><td style=\"text-align: right;\">  0.515515</td><td style=\"text-align: right;\">  0.657261</td><td style=\"text-align: right;\">   0.302791</td><td style=\"text-align: right;\">   0.960396</td><td style=\"text-align: right;\">   0.631798</td><td style=\"text-align: right;\">   0.893669</td><td style=\"text-align: right;\">   0.493319</td><td style=\"text-align: right;\">   0.414794</td><td style=\"text-align: right;\">   0.423718</td><td style=\"text-align: right;\">   0.250772</td><td style=\"text-align: right;\">   0.623676</td><td style=\"text-align: right;\">   0.402197</td><td style=\"text-align: right;\">   0.366951</td><td style=\"text-align: right;\">   0.856432</td><td style=\"text-align: right;\">   0.416419</td><td style=\"text-align: right;\">   0.253613</td><td style=\"text-align: right;\">   0.438635</td><td style=\"text-align: right;\">   0.466033</td><td style=\"text-align: right;\">   0.736525</td><td style=\"text-align: right;\">   0.513399</td><td style=\"text-align: right;\">   0.801483</td><td style=\"text-align: right;\">  0.0970547</td><td style=\"text-align: right;\">   0.795364</td><td style=\"text-align: right;\">   0.76499 </td><td style=\"text-align: right;\">   0.488171</td><td style=\"text-align: right;\">   0.361662</td><td style=\"text-align: right;\">   0.741337</td><td style=\"text-align: right;\">   0.622716</td><td style=\"text-align: right;\">   0.131369</td><td style=\"text-align: right;\">   0.328701</td><td style=\"text-align: right;\">   0.478535</td><td style=\"text-align: right;\">   0.678194</td><td style=\"text-align: right;\">   0.633156</td><td style=\"text-align: right;\">   0.535449</td><td style=\"text-align: right;\">   0.360559</td><td style=\"text-align: right;\">   0.622619</td><td style=\"text-align: right;\">   0.57833 </td><td style=\"text-align: right;\">   0.540323</td><td style=\"text-align: right;\">   0.742386</td><td style=\"text-align: right;\">   0.530327</td><td style=\"text-align: right;\">  0.0474118</td><td style=\"text-align: right;\">   0.243059</td><td style=\"text-align: right;\">   0.166923</td><td style=\"text-align: right;\">   0.643619</td><td style=\"text-align: right;\">   0.565637</td><td style=\"text-align: right;\">   0.485153</td><td style=\"text-align: right;\">   0.159425</td><td style=\"text-align: right;\">   0.447211</td><td style=\"text-align: right;\">   0.643797</td><td style=\"text-align: right;\">   0.398147</td><td style=\"text-align: right;\">   0.714722</td><td style=\"text-align: right;\">   0.22663 </td><td style=\"text-align: right;\">   0.232906</td><td style=\"text-align: right;\">  0.793634 </td><td style=\"text-align: right;\">   0.844612</td><td style=\"text-align: right;\">   0.82989 </td><td style=\"text-align: right;\">   0.454821</td><td style=\"text-align: right;\"> 0.00313339</td><td style=\"text-align: right;\">   0.470301</td><td style=\"text-align: right;\">   0.642782</td><td style=\"text-align: right;\">   0.593543</td><td style=\"text-align: right;\">   0.561413</td><td style=\"text-align: right;\">   0.600574</td><td style=\"text-align: right;\">   0.572941</td><td style=\"text-align: right;\">   0.239407</td><td style=\"text-align: right;\">   0.387228</td><td style=\"text-align: right;\">   0.517453</td><td style=\"text-align: right;\">   0.680282</td><td style=\"text-align: right;\">   0.614126</td><td style=\"text-align: right;\">   0.744267</td><td style=\"text-align: right;\">   0.459505</td><td style=\"text-align: right;\">   0.624555</td><td style=\"text-align: right;\">   0.192916</td><td style=\"text-align: right;\">   0.353526</td><td style=\"text-align: right;\">   0.972903</td><td style=\"text-align: right;\">   0.187681</td><td style=\"text-align: right;\">   0.610783</td><td style=\"text-align: right;\">   0.529083</td><td style=\"text-align: right;\">   0.515937</td><td style=\"text-align: right;\">   0.524104</td><td style=\"text-align: right;\">   0.475644</td><td style=\"text-align: right;\">   0.247978</td><td style=\"text-align: right;\">   0.408162</td><td style=\"text-align: right;\">   0.156225</td><td style=\"text-align: right;\">  0.0841352</td><td style=\"text-align: right;\">   0.29291 </td><td style=\"text-align: right;\">   0.45292 </td><td style=\"text-align: right;\">   0.594526</td><td style=\"text-align: right;\">   0.231825</td><td style=\"text-align: right;\">   0.367741</td><td style=\"text-align: right;\">   0.606268</td><td style=\"text-align: right;\">   0.553196</td><td style=\"text-align: right;\">    0.432877</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.487941</td><td style=\"text-align: right;\">  0.120817</td><td style=\"text-align: right;\">  0.498751</td><td style=\"text-align: right;\">  0.678203</td><td style=\"text-align: right;\">  0.582468</td><td style=\"text-align: right;\"> 0.0675392</td><td style=\"text-align: right;\">  0.409305</td><td style=\"text-align: right;\">  0.522305</td><td style=\"text-align: right;\">  0.665502</td><td style=\"text-align: right;\">   0.297582</td><td style=\"text-align: right;\">   0.974413</td><td style=\"text-align: right;\">   0.631207</td><td style=\"text-align: right;\">   0.907878</td><td style=\"text-align: right;\">   0.493883</td><td style=\"text-align: right;\">   0.413411</td><td style=\"text-align: right;\">   0.416253</td><td style=\"text-align: right;\">   0.245957</td><td style=\"text-align: right;\">   0.626557</td><td style=\"text-align: right;\">   0.401309</td><td style=\"text-align: right;\">   0.363863</td><td style=\"text-align: right;\">   0.869916</td><td style=\"text-align: right;\">   0.412246</td><td style=\"text-align: right;\">   0.247264</td><td style=\"text-align: right;\">   0.437452</td><td style=\"text-align: right;\">   0.462213</td><td style=\"text-align: right;\">   0.741952</td><td style=\"text-align: right;\">   0.51227 </td><td style=\"text-align: right;\">   0.811521</td><td style=\"text-align: right;\">  0.0840566</td><td style=\"text-align: right;\">   0.808053</td><td style=\"text-align: right;\">   0.77725 </td><td style=\"text-align: right;\">   0.490589</td><td style=\"text-align: right;\">   0.357131</td><td style=\"text-align: right;\">   0.749177</td><td style=\"text-align: right;\">   0.626842</td><td style=\"text-align: right;\">   0.121537</td><td style=\"text-align: right;\">   0.32413 </td><td style=\"text-align: right;\">   0.472436</td><td style=\"text-align: right;\">   0.682859</td><td style=\"text-align: right;\">   0.633624</td><td style=\"text-align: right;\">   0.539367</td><td style=\"text-align: right;\">   0.358953</td><td style=\"text-align: right;\">   0.627207</td><td style=\"text-align: right;\">   0.579838</td><td style=\"text-align: right;\">   0.540836</td><td style=\"text-align: right;\">   0.757461</td><td style=\"text-align: right;\">   0.527317</td><td style=\"text-align: right;\">  0.0344178</td><td style=\"text-align: right;\">   0.233201</td><td style=\"text-align: right;\">   0.155818</td><td style=\"text-align: right;\">   0.649326</td><td style=\"text-align: right;\">   0.566264</td><td style=\"text-align: right;\">   0.485642</td><td style=\"text-align: right;\">   0.150823</td><td style=\"text-align: right;\">   0.4515  </td><td style=\"text-align: right;\">   0.644643</td><td style=\"text-align: right;\">   0.395019</td><td style=\"text-align: right;\">   0.716242</td><td style=\"text-align: right;\">   0.218618</td><td style=\"text-align: right;\">   0.223877</td><td style=\"text-align: right;\">  0.804296 </td><td style=\"text-align: right;\">   0.859872</td><td style=\"text-align: right;\">   0.843241</td><td style=\"text-align: right;\">   0.458465</td><td style=\"text-align: right;\"> 0         </td><td style=\"text-align: right;\">   0.468991</td><td style=\"text-align: right;\">   0.642549</td><td style=\"text-align: right;\">   0.597488</td><td style=\"text-align: right;\">   0.56041 </td><td style=\"text-align: right;\">   0.607344</td><td style=\"text-align: right;\">   0.575388</td><td style=\"text-align: right;\">   0.230257</td><td style=\"text-align: right;\">   0.379388</td><td style=\"text-align: right;\">   0.513558</td><td style=\"text-align: right;\">   0.686377</td><td style=\"text-align: right;\">   0.621748</td><td style=\"text-align: right;\">   0.749322</td><td style=\"text-align: right;\">   0.456526</td><td style=\"text-align: right;\">   0.630173</td><td style=\"text-align: right;\">   0.184632</td><td style=\"text-align: right;\">   0.348653</td><td style=\"text-align: right;\">   0.990128</td><td style=\"text-align: right;\">   0.176419</td><td style=\"text-align: right;\">   0.610627</td><td style=\"text-align: right;\">   0.532671</td><td style=\"text-align: right;\">   0.517664</td><td style=\"text-align: right;\">   0.524954</td><td style=\"text-align: right;\">   0.479827</td><td style=\"text-align: right;\">   0.241617</td><td style=\"text-align: right;\">   0.408542</td><td style=\"text-align: right;\">   0.146407</td><td style=\"text-align: right;\">  0.0736812</td><td style=\"text-align: right;\">   0.284788</td><td style=\"text-align: right;\">   0.448157</td><td style=\"text-align: right;\">   0.598885</td><td style=\"text-align: right;\">   0.223648</td><td style=\"text-align: right;\">   0.36858 </td><td style=\"text-align: right;\">   0.611246</td><td style=\"text-align: right;\">   0.553916</td><td style=\"text-align: right;\">    0.434304</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.484816</td><td style=\"text-align: right;\">  0.139675</td><td style=\"text-align: right;\">  0.485834</td><td style=\"text-align: right;\">  0.672652</td><td style=\"text-align: right;\">  0.571843</td><td style=\"text-align: right;\"> 0.0929544</td><td style=\"text-align: right;\">  0.40756 </td><td style=\"text-align: right;\">  0.507277</td><td style=\"text-align: right;\">  0.647449</td><td style=\"text-align: right;\">   0.308351</td><td style=\"text-align: right;\">   0.944789</td><td style=\"text-align: right;\">   0.633097</td><td style=\"text-align: right;\">   0.877564</td><td style=\"text-align: right;\">   0.49294 </td><td style=\"text-align: right;\">   0.41627 </td><td style=\"text-align: right;\">   0.432531</td><td style=\"text-align: right;\">   0.255841</td><td style=\"text-align: right;\">   0.62027 </td><td style=\"text-align: right;\">   0.40318 </td><td style=\"text-align: right;\">   0.369982</td><td style=\"text-align: right;\">   0.841401</td><td style=\"text-align: right;\">   0.420747</td><td style=\"text-align: right;\">   0.260932</td><td style=\"text-align: right;\">   0.44012 </td><td style=\"text-align: right;\">   0.470406</td><td style=\"text-align: right;\">   0.730344</td><td style=\"text-align: right;\">   0.514256</td><td style=\"text-align: right;\">   0.789997</td><td style=\"text-align: right;\">  0.111695 </td><td style=\"text-align: right;\">   0.780912</td><td style=\"text-align: right;\">   0.750874</td><td style=\"text-align: right;\">   0.485223</td><td style=\"text-align: right;\">   0.367147</td><td style=\"text-align: right;\">   0.73256 </td><td style=\"text-align: right;\">   0.618408</td><td style=\"text-align: right;\">   0.142117</td><td style=\"text-align: right;\">   0.333414</td><td style=\"text-align: right;\">   0.486052</td><td style=\"text-align: right;\">   0.673358</td><td style=\"text-align: right;\">   0.632934</td><td style=\"text-align: right;\">   0.530601</td><td style=\"text-align: right;\">   0.362174</td><td style=\"text-align: right;\">   0.617323</td><td style=\"text-align: right;\">   0.577164</td><td style=\"text-align: right;\">   0.540219</td><td style=\"text-align: right;\">   0.724778</td><td style=\"text-align: right;\">   0.533898</td><td style=\"text-align: right;\">  0.0619547</td><td style=\"text-align: right;\">   0.254547</td><td style=\"text-align: right;\">   0.179372</td><td style=\"text-align: right;\">   0.637335</td><td style=\"text-align: right;\">   0.565316</td><td style=\"text-align: right;\">   0.484893</td><td style=\"text-align: right;\">   0.168939</td><td style=\"text-align: right;\">   0.441874</td><td style=\"text-align: right;\">   0.642964</td><td style=\"text-align: right;\">   0.401418</td><td style=\"text-align: right;\">   0.713493</td><td style=\"text-align: right;\">   0.235908</td><td style=\"text-align: right;\">   0.243545</td><td style=\"text-align: right;\">  0.781417 </td><td style=\"text-align: right;\">   0.827211</td><td style=\"text-align: right;\">   0.814484</td><td style=\"text-align: right;\">   0.45038 </td><td style=\"text-align: right;\"> 0.019522  </td><td style=\"text-align: right;\">   0.471379</td><td style=\"text-align: right;\">   0.643177</td><td style=\"text-align: right;\">   0.589026</td><td style=\"text-align: right;\">   0.56292 </td><td style=\"text-align: right;\">   0.592443</td><td style=\"text-align: right;\">   0.570098</td><td style=\"text-align: right;\">   0.249546</td><td style=\"text-align: right;\">   0.396457</td><td style=\"text-align: right;\">   0.522005</td><td style=\"text-align: right;\">   0.673301</td><td style=\"text-align: right;\">   0.605535</td><td style=\"text-align: right;\">   0.738631</td><td style=\"text-align: right;\">   0.463133</td><td style=\"text-align: right;\">   0.618176</td><td style=\"text-align: right;\">   0.202261</td><td style=\"text-align: right;\">   0.359044</td><td style=\"text-align: right;\">   0.952916</td><td style=\"text-align: right;\">   0.200504</td><td style=\"text-align: right;\">   0.611546</td><td style=\"text-align: right;\">   0.5246  </td><td style=\"text-align: right;\">   0.513824</td><td style=\"text-align: right;\">   0.522946</td><td style=\"text-align: right;\">   0.470473</td><td style=\"text-align: right;\">   0.255027</td><td style=\"text-align: right;\">   0.407578</td><td style=\"text-align: right;\">   0.167444</td><td style=\"text-align: right;\">  0.09575  </td><td style=\"text-align: right;\">   0.302409</td><td style=\"text-align: right;\">   0.458296</td><td style=\"text-align: right;\">   0.589436</td><td style=\"text-align: right;\">   0.240997</td><td style=\"text-align: right;\">   0.366431</td><td style=\"text-align: right;\">   0.600828</td><td style=\"text-align: right;\">   0.55295 </td><td style=\"text-align: right;\">    0.431083</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.455044</td><td style=\"text-align: right;\">  0.51649 </td><td style=\"text-align: right;\">  0.338442</td><td style=\"text-align: right;\">  0.532032</td><td style=\"text-align: right;\">  0.416448</td><td style=\"text-align: right;\"> 0.566405 </td><td style=\"text-align: right;\">  0.426066</td><td style=\"text-align: right;\">  0.323111</td><td style=\"text-align: right;\">  0.370865</td><td style=\"text-align: right;\">   0.518038</td><td style=\"text-align: right;\">   0.395768</td><td style=\"text-align: right;\">   0.596197</td><td style=\"text-align: right;\">   0.366281</td><td style=\"text-align: right;\">   0.488181</td><td style=\"text-align: right;\">   0.482539</td><td style=\"text-align: right;\">   0.659195</td><td style=\"text-align: right;\">   0.477544</td><td style=\"text-align: right;\">   0.493709</td><td style=\"text-align: right;\">   0.469993</td><td style=\"text-align: right;\">   0.493987</td><td style=\"text-align: right;\">   0.35874 </td><td style=\"text-align: right;\">   0.551969</td><td style=\"text-align: right;\">   0.526724</td><td style=\"text-align: right;\">   0.496622</td><td style=\"text-align: right;\">   0.581186</td><td style=\"text-align: right;\">   0.501548</td><td style=\"text-align: right;\">   0.528723</td><td style=\"text-align: right;\">   0.419457</td><td style=\"text-align: right;\">  0.599608 </td><td style=\"text-align: right;\">   0.3397  </td><td style=\"text-align: right;\">   0.328944</td><td style=\"text-align: right;\">   0.423878</td><td style=\"text-align: right;\">   0.545505</td><td style=\"text-align: right;\">   0.434884</td><td style=\"text-align: right;\">   0.469824</td><td style=\"text-align: right;\">   0.537103</td><td style=\"text-align: right;\">   0.51405 </td><td style=\"text-align: right;\">   0.660447</td><td style=\"text-align: right;\">   0.495073</td><td style=\"text-align: right;\">   0.569309</td><td style=\"text-align: right;\">   0.412095</td><td style=\"text-align: right;\">   0.461178</td><td style=\"text-align: right;\">   0.44819 </td><td style=\"text-align: right;\">   0.51852 </td><td style=\"text-align: right;\">   0.519953</td><td style=\"text-align: right;\">   0.237176</td><td style=\"text-align: right;\">   0.600504</td><td style=\"text-align: right;\">  0.574394 </td><td style=\"text-align: right;\">   0.612161</td><td style=\"text-align: right;\">   0.58938 </td><td style=\"text-align: right;\">   0.440622</td><td style=\"text-align: right;\">   0.533637</td><td style=\"text-align: right;\">   0.482311</td><td style=\"text-align: right;\">   0.521647</td><td style=\"text-align: right;\">   0.350391</td><td style=\"text-align: right;\">   0.56088 </td><td style=\"text-align: right;\">   0.51875 </td><td style=\"text-align: right;\">   0.602379</td><td style=\"text-align: right;\">   0.557209</td><td style=\"text-align: right;\">   0.586098</td><td style=\"text-align: right;\">  0.394645 </td><td style=\"text-align: right;\">   0.302302</td><td style=\"text-align: right;\">   0.33949 </td><td style=\"text-align: right;\">   0.376576</td><td style=\"text-align: right;\"> 0.590604  </td><td style=\"text-align: right;\">   0.508353</td><td style=\"text-align: right;\">   0.599316</td><td style=\"text-align: right;\">   0.448242</td><td style=\"text-align: right;\">   0.568337</td><td style=\"text-align: right;\">   0.374294</td><td style=\"text-align: right;\">   0.473156</td><td style=\"text-align: right;\">   0.582113</td><td style=\"text-align: right;\">   0.644425</td><td style=\"text-align: right;\">   0.61697 </td><td style=\"text-align: right;\">   0.444832</td><td style=\"text-align: right;\">   0.37059 </td><td style=\"text-align: right;\">   0.517235</td><td style=\"text-align: right;\">   0.559615</td><td style=\"text-align: right;\">   0.4263  </td><td style=\"text-align: right;\">   0.536257</td><td style=\"text-align: right;\">   0.537455</td><td style=\"text-align: right;\">   0.312137</td><td style=\"text-align: right;\">   0.614255</td><td style=\"text-align: right;\">   0.583118</td><td style=\"text-align: right;\">   0.414624</td><td style=\"text-align: right;\">   0.456833</td><td style=\"text-align: right;\">   0.491798</td><td style=\"text-align: right;\">   0.373546</td><td style=\"text-align: right;\">   0.515204</td><td style=\"text-align: right;\">   0.43441 </td><td style=\"text-align: right;\">   0.556642</td><td style=\"text-align: right;\">  0.525152 </td><td style=\"text-align: right;\">   0.598192</td><td style=\"text-align: right;\">   0.59739 </td><td style=\"text-align: right;\">   0.438784</td><td style=\"text-align: right;\">   0.55726 </td><td style=\"text-align: right;\">   0.398072</td><td style=\"text-align: right;\">   0.439151</td><td style=\"text-align: right;\">   0.525794</td><td style=\"text-align: right;\">    0.418375</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.476391</td><td style=\"text-align: right;\">  0.215299</td><td style=\"text-align: right;\">  0.447875</td><td style=\"text-align: right;\">  0.646344</td><td style=\"text-align: right;\">  0.536371</td><td style=\"text-align: right;\"> 0.190282 </td><td style=\"text-align: right;\">  0.406907</td><td style=\"text-align: right;\">  0.46086 </td><td style=\"text-align: right;\">  0.585135</td><td style=\"text-align: right;\">   0.35051 </td><td style=\"text-align: right;\">   0.832047</td><td style=\"text-align: right;\">   0.631573</td><td style=\"text-align: right;\">   0.768322</td><td style=\"text-align: right;\">   0.491605</td><td style=\"text-align: right;\">   0.428829</td><td style=\"text-align: right;\">   0.485682</td><td style=\"text-align: right;\">   0.298428</td><td style=\"text-align: right;\">   0.593862</td><td style=\"text-align: right;\">   0.415003</td><td style=\"text-align: right;\">   0.393835</td><td style=\"text-align: right;\">   0.738937</td><td style=\"text-align: right;\">   0.448979</td><td style=\"text-align: right;\">   0.315374</td><td style=\"text-align: right;\">   0.451837</td><td style=\"text-align: right;\">   0.496298</td><td style=\"text-align: right;\">   0.683853</td><td style=\"text-align: right;\">   0.517839</td><td style=\"text-align: right;\">   0.711098</td><td style=\"text-align: right;\">  0.214079 </td><td style=\"text-align: right;\">   0.685139</td><td style=\"text-align: right;\">   0.658149</td><td style=\"text-align: right;\">   0.469251</td><td style=\"text-align: right;\">   0.405316</td><td style=\"text-align: right;\">   0.670502</td><td style=\"text-align: right;\">   0.58769 </td><td style=\"text-align: right;\">   0.22207 </td><td style=\"text-align: right;\">   0.369301</td><td style=\"text-align: right;\">   0.528945</td><td style=\"text-align: right;\">   0.637715</td><td style=\"text-align: right;\">   0.62453 </td><td style=\"text-align: right;\">   0.501786</td><td style=\"text-align: right;\">   0.379778</td><td style=\"text-align: right;\">   0.581081</td><td style=\"text-align: right;\">   0.566835</td><td style=\"text-align: right;\">   0.537848</td><td style=\"text-align: right;\">   0.61423 </td><td style=\"text-align: right;\">   0.551758</td><td style=\"text-align: right;\">  0.167577 </td><td style=\"text-align: right;\">   0.332238</td><td style=\"text-align: right;\">   0.265521</td><td style=\"text-align: right;\">   0.595376</td><td style=\"text-align: right;\">   0.560902</td><td style=\"text-align: right;\">   0.484225</td><td style=\"text-align: right;\">   0.239794</td><td style=\"text-align: right;\">   0.415452</td><td style=\"text-align: right;\">   0.630104</td><td style=\"text-align: right;\">   0.425066</td><td style=\"text-align: right;\">   0.695667</td><td style=\"text-align: right;\">   0.303165</td><td style=\"text-align: right;\">   0.317164</td><td style=\"text-align: right;\">  0.698366 </td><td style=\"text-align: right;\">   0.712839</td><td style=\"text-align: right;\">   0.711333</td><td style=\"text-align: right;\">   0.429154</td><td style=\"text-align: right;\"> 0.137135  </td><td style=\"text-align: right;\">   0.47876 </td><td style=\"text-align: right;\">   0.638666</td><td style=\"text-align: right;\">   0.558502</td><td style=\"text-align: right;\">   0.567994</td><td style=\"text-align: right;\">   0.541749</td><td style=\"text-align: right;\">   0.5492  </td><td style=\"text-align: right;\">   0.319225</td><td style=\"text-align: right;\">   0.45321 </td><td style=\"text-align: right;\">   0.546294</td><td style=\"text-align: right;\">   0.624472</td><td style=\"text-align: right;\">   0.552451</td><td style=\"text-align: right;\">   0.694531</td><td style=\"text-align: right;\">   0.485864</td><td style=\"text-align: right;\">   0.575914</td><td style=\"text-align: right;\">   0.271084</td><td style=\"text-align: right;\">   0.396561</td><td style=\"text-align: right;\">   0.815449</td><td style=\"text-align: right;\">   0.288518</td><td style=\"text-align: right;\">   0.610304</td><td style=\"text-align: right;\">   0.498167</td><td style=\"text-align: right;\">   0.500145</td><td style=\"text-align: right;\">   0.515981</td><td style=\"text-align: right;\">   0.44436 </td><td style=\"text-align: right;\">   0.307896</td><td style=\"text-align: right;\">   0.409248</td><td style=\"text-align: right;\">   0.248502</td><td style=\"text-align: right;\">  0.182826 </td><td style=\"text-align: right;\">   0.366474</td><td style=\"text-align: right;\">   0.490438</td><td style=\"text-align: right;\">   0.555791</td><td style=\"text-align: right;\">   0.306616</td><td style=\"text-align: right;\">   0.36726 </td><td style=\"text-align: right;\">   0.566034</td><td style=\"text-align: right;\">   0.549495</td><td style=\"text-align: right;\">    0.424631</td></tr>\n",
              "</tbody>\n",
              "</table><pre style='font-size: smaller; margin-bottom: 1em;'>[10040 rows x 100 columns]</pre>"
            ],
            "text/plain": [
              "  DF.L1.C1    DF.L1.C2    DF.L1.C3    DF.L1.C4    DF.L1.C5    DF.L1.C6    DF.L1.C7    DF.L1.C8    DF.L1.C9    DF.L1.C10    DF.L1.C11    DF.L1.C12    DF.L1.C13    DF.L1.C14    DF.L1.C15    DF.L1.C16    DF.L1.C17    DF.L1.C18    DF.L1.C19    DF.L1.C20    DF.L1.C21    DF.L1.C22    DF.L1.C23    DF.L1.C24    DF.L1.C25    DF.L1.C26    DF.L1.C27    DF.L1.C28    DF.L1.C29    DF.L1.C30    DF.L1.C31    DF.L1.C32    DF.L1.C33    DF.L1.C34    DF.L1.C35    DF.L1.C36    DF.L1.C37    DF.L1.C38    DF.L1.C39    DF.L1.C40    DF.L1.C41    DF.L1.C42    DF.L1.C43    DF.L1.C44    DF.L1.C45    DF.L1.C46    DF.L1.C47    DF.L1.C48    DF.L1.C49    DF.L1.C50    DF.L1.C51    DF.L1.C52    DF.L1.C53    DF.L1.C54    DF.L1.C55    DF.L1.C56    DF.L1.C57    DF.L1.C58    DF.L1.C59    DF.L1.C60    DF.L1.C61    DF.L1.C62    DF.L1.C63    DF.L1.C64    DF.L1.C65    DF.L1.C66    DF.L1.C67    DF.L1.C68    DF.L1.C69    DF.L1.C70    DF.L1.C71    DF.L1.C72    DF.L1.C73    DF.L1.C74    DF.L1.C75    DF.L1.C76    DF.L1.C77    DF.L1.C78    DF.L1.C79    DF.L1.C80    DF.L1.C81    DF.L1.C82    DF.L1.C83    DF.L1.C84    DF.L1.C85    DF.L1.C86    DF.L1.C87    DF.L1.C88    DF.L1.C89    DF.L1.C90    DF.L1.C91    DF.L1.C92    DF.L1.C93    DF.L1.C94    DF.L1.C95    DF.L1.C96    DF.L1.C97    DF.L1.C98    DF.L1.C99    DF.L1.C100\n",
              "----------  ----------  ----------  ----------  ----------  ----------  ----------  ----------  ----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  ------------\n",
              "  0.476866    0.206894    0.450783    0.649465    0.539621   0.179717     0.40628     0.464371    0.591009     0.345847     0.844423     0.632539     0.779635     0.491806     0.427308     0.480988     0.293482     0.596271     0.413662     0.390864     0.749717     0.445975     0.309583     0.450759     0.493889     0.688875     0.517312     0.719213    0.20324       0.694822     0.667202     0.470412     0.401534     0.677126     0.591005     0.213191     0.365171     0.525412     0.64191      0.62637      0.50403      0.377655     0.584738     0.568365     0.538596     0.62458      0.550305    0.156256      0.324684     0.256291     0.599857     0.561941     0.484416     0.231673     0.417029     0.632142     0.422291     0.698298     0.296374     0.309958    0.706764      0.724398     0.72156      0.43064    0.124262       0.477825     0.639924     0.561492     0.568198     0.546074     0.550986     0.311592     0.447869     0.544299     0.629252     0.557639     0.699405     0.484068     0.57992      0.263858     0.392426     0.829131     0.279419     0.611305     0.500395     0.501034     0.516754     0.446399     0.302181     0.408349     0.2401      0.173322      0.360124     0.487374     0.55881      0.299605     0.366409     0.569901     0.550494      0.424736\n",
              "  0.477317    0.280099    0.441929    0.617288    0.519714   0.266042     0.41942     0.450049    0.552145     0.385475     0.744338     0.612708     0.696234     0.492979     0.441851     0.507381     0.340426     0.573147     0.430916     0.415872     0.670374     0.465169     0.359586     0.4616       0.506406     0.6452       0.516537     0.657574    0.287249      0.626406     0.603958     0.466854     0.432003     0.625039     0.565623     0.287412     0.399658     0.541953     0.608491     0.604101     0.491838     0.402203     0.556971     0.555472     0.532367     0.559124     0.552426    0.249121      0.381221     0.326268     0.568427     0.552486     0.48559      0.2997       0.417678     0.607292     0.443875     0.666979     0.353408     0.366634    0.644096      0.644065     0.647085     0.431369   0.226893       0.483456     0.620899     0.539412     0.560657     0.518868     0.535126     0.368285     0.480039     0.550057     0.591539     0.52698      0.65535      0.495579     0.551691     0.324845     0.422938     0.723827     0.348193     0.596786     0.488226     0.49475      0.51174      0.442767     0.351048     0.422419     0.309533    0.254511      0.407451     0.503698     0.537192     0.355856     0.385372     0.545199     0.542581      0.431506\n",
              "  0.430792    0.844291    0.21881     0.407217    0.285391   0.97523      0.447026    0.171927    0.138119     0.701054     0            0.556361     0            0.484468     0.540933     0.848335     0.6734       0.382539     0.531024     0.602757     0            0.664165     0.756833     0.546111     0.673279     0.303067     0.540185     0.10241     1.01847       0            0            0.374151     0.697945     0.178785     0.3409       0.879865     0.67191      0.803213     0.34038      0.510869     0.313586     0.551029     0.303553     0.465938     0.501075     0            0.65273     1.01729       0.917633     0.940552     0.272944     0.505037     0.480383     0.826736     0.278753     0.486329     0.620161     0.50022      0.83495      0.87961     0.0640937     0            0            0.319953   1.08314        0.540833     0.557628     0.327804     0.569481     0.190697     0.388302     0.866509     0.851718     0.693611     0.248035     0.173231     0.323887     0.641123     0.262343     0.825806     0.689262     0            0.968292     0.554018     0.323948     0.407817     0.466666     0.297718     0.741396     0.460532     0.892748    0.898098      0.850295     0.713407     0.310118     0.829867     0.432174     0.302838     0.500999      0.411288\n",
              "  0.475253    0.247348    0.44025     0.634063    0.525619   0.229905     0.41182     0.452342    0.566636     0.369103     0.785181     0.624456     0.727473     0.489785     0.435017     0.499991     0.318573     0.586294     0.42007      0.407143     0.698772     0.461484     0.336032     0.454937     0.504409     0.66553      0.521613     0.682302    0.253943      0.650813     0.627196     0.466635     0.417365     0.645862     0.574131     0.256574     0.386629     0.537966     0.620042     0.615127     0.496441     0.388659     0.568402     0.558507     0.532509     0.58059      0.5558      0.209727      0.357494     0.299882     0.578258     0.554717     0.482209     0.271287     0.413427     0.621137     0.43666      0.683072     0.326602     0.340926    0.668888      0.671938     0.676193     0.426081   0.186373       0.483958     0.632354     0.548167     0.564677     0.529386     0.543687     0.348359     0.47036      0.552165     0.607978     0.534012     0.67612      0.490463     0.562124     0.297452     0.411947     0.768756     0.321116     0.603064     0.492511     0.498419     0.513818     0.439645     0.329483     0.414258     0.278461    0.218764      0.387821     0.501183     0.546238     0.332626     0.372885     0.550831     0.542662      0.425819\n",
              "  0.476208    0.207308    0.448446    0.6504      0.538316   0.181371     0.405701    0.462882    0.589632     0.346635     0.841904     0.633728     0.776401     0.490537     0.427273     0.482895     0.293303     0.597817     0.412167     0.392269     0.745979     0.448354     0.309235     0.449832     0.495577     0.689044     0.519612     0.717642    0.205679      0.691676     0.664792     0.469723     0.401132     0.675692     0.589608     0.214738     0.366501     0.526852     0.640001     0.626353     0.503886     0.376494     0.584204     0.566908     0.537131     0.620991     0.552314    0.157681      0.325394     0.259061     0.597649     0.560697     0.48303      0.233245     0.415582     0.633058     0.423659     0.699009     0.29579      0.309788    0.705081      0.720301     0.719469     0.428452   0.127445       0.479283     0.640903     0.560981     0.568193     0.5456       0.551936     0.314632     0.450124     0.5464       0.629126     0.554424     0.699704     0.483842     0.579009     0.26395      0.394005     0.827994     0.281434     0.610858     0.499927     0.501753     0.516525     0.44422      0.3025       0.407853     0.240263    0.173972      0.360941     0.489643     0.558706     0.300787     0.364753     0.567018     0.548561      0.423698\n",
              "  0.486466    0.129757    0.492849    0.675551    0.577524   0.0795401    0.408679    0.515515    0.657261     0.302791     0.960396     0.631798     0.893669     0.493319     0.414794     0.423718     0.250772     0.623676     0.402197     0.366951     0.856432     0.416419     0.253613     0.438635     0.466033     0.736525     0.513399     0.801483    0.0970547     0.795364     0.76499      0.488171     0.361662     0.741337     0.622716     0.131369     0.328701     0.478535     0.678194     0.633156     0.535449     0.360559     0.622619     0.57833      0.540323     0.742386     0.530327    0.0474118     0.243059     0.166923     0.643619     0.565637     0.485153     0.159425     0.447211     0.643797     0.398147     0.714722     0.22663      0.232906    0.793634      0.844612     0.82989      0.454821   0.00313339     0.470301     0.642782     0.593543     0.561413     0.600574     0.572941     0.239407     0.387228     0.517453     0.680282     0.614126     0.744267     0.459505     0.624555     0.192916     0.353526     0.972903     0.187681     0.610783     0.529083     0.515937     0.524104     0.475644     0.247978     0.408162     0.156225    0.0841352     0.29291      0.45292      0.594526     0.231825     0.367741     0.606268     0.553196      0.432877\n",
              "  0.487941    0.120817    0.498751    0.678203    0.582468   0.0675392    0.409305    0.522305    0.665502     0.297582     0.974413     0.631207     0.907878     0.493883     0.413411     0.416253     0.245957     0.626557     0.401309     0.363863     0.869916     0.412246     0.247264     0.437452     0.462213     0.741952     0.51227      0.811521    0.0840566     0.808053     0.77725      0.490589     0.357131     0.749177     0.626842     0.121537     0.32413      0.472436     0.682859     0.633624     0.539367     0.358953     0.627207     0.579838     0.540836     0.757461     0.527317    0.0344178     0.233201     0.155818     0.649326     0.566264     0.485642     0.150823     0.4515       0.644643     0.395019     0.716242     0.218618     0.223877    0.804296      0.859872     0.843241     0.458465   0              0.468991     0.642549     0.597488     0.56041      0.607344     0.575388     0.230257     0.379388     0.513558     0.686377     0.621748     0.749322     0.456526     0.630173     0.184632     0.348653     0.990128     0.176419     0.610627     0.532671     0.517664     0.524954     0.479827     0.241617     0.408542     0.146407    0.0736812     0.284788     0.448157     0.598885     0.223648     0.36858      0.611246     0.553916      0.434304\n",
              "  0.484816    0.139675    0.485834    0.672652    0.571843   0.0929544    0.40756     0.507277    0.647449     0.308351     0.944789     0.633097     0.877564     0.49294      0.41627      0.432531     0.255841     0.62027      0.40318      0.369982     0.841401     0.420747     0.260932     0.44012      0.470406     0.730344     0.514256     0.789997    0.111695      0.780912     0.750874     0.485223     0.367147     0.73256      0.618408     0.142117     0.333414     0.486052     0.673358     0.632934     0.530601     0.362174     0.617323     0.577164     0.540219     0.724778     0.533898    0.0619547     0.254547     0.179372     0.637335     0.565316     0.484893     0.168939     0.441874     0.642964     0.401418     0.713493     0.235908     0.243545    0.781417      0.827211     0.814484     0.45038    0.019522       0.471379     0.643177     0.589026     0.56292      0.592443     0.570098     0.249546     0.396457     0.522005     0.673301     0.605535     0.738631     0.463133     0.618176     0.202261     0.359044     0.952916     0.200504     0.611546     0.5246       0.513824     0.522946     0.470473     0.255027     0.407578     0.167444    0.09575       0.302409     0.458296     0.589436     0.240997     0.366431     0.600828     0.55295       0.431083\n",
              "  0.455044    0.51649     0.338442    0.532032    0.416448   0.566405     0.426066    0.323111    0.370865     0.518038     0.395768     0.596197     0.366281     0.488181     0.482539     0.659195     0.477544     0.493709     0.469993     0.493987     0.35874      0.551969     0.526724     0.496622     0.581186     0.501548     0.528723     0.419457    0.599608      0.3397       0.328944     0.423878     0.545505     0.434884     0.469824     0.537103     0.51405      0.660447     0.495073     0.569309     0.412095     0.461178     0.44819      0.51852      0.519953     0.237176     0.600504    0.574394      0.612161     0.58938      0.440622     0.533637     0.482311     0.521647     0.350391     0.56088      0.51875      0.602379     0.557209     0.586098    0.394645      0.302302     0.33949      0.376576   0.590604       0.508353     0.599316     0.448242     0.568337     0.374294     0.473156     0.582113     0.644425     0.61697      0.444832     0.37059      0.517235     0.559615     0.4263       0.536257     0.537455     0.312137     0.614255     0.583118     0.414624     0.456833     0.491798     0.373546     0.515204     0.43441      0.556642    0.525152      0.598192     0.59739      0.438784     0.55726      0.398072     0.439151     0.525794      0.418375\n",
              "  0.476391    0.215299    0.447875    0.646344    0.536371   0.190282     0.406907    0.46086     0.585135     0.35051      0.832047     0.631573     0.768322     0.491605     0.428829     0.485682     0.298428     0.593862     0.415003     0.393835     0.738937     0.448979     0.315374     0.451837     0.496298     0.683853     0.517839     0.711098    0.214079      0.685139     0.658149     0.469251     0.405316     0.670502     0.58769      0.22207      0.369301     0.528945     0.637715     0.62453      0.501786     0.379778     0.581081     0.566835     0.537848     0.61423      0.551758    0.167577      0.332238     0.265521     0.595376     0.560902     0.484225     0.239794     0.415452     0.630104     0.425066     0.695667     0.303165     0.317164    0.698366      0.712839     0.711333     0.429154   0.137135       0.47876      0.638666     0.558502     0.567994     0.541749     0.5492       0.319225     0.45321      0.546294     0.624472     0.552451     0.694531     0.485864     0.575914     0.271084     0.396561     0.815449     0.288518     0.610304     0.498167     0.500145     0.515981     0.44436      0.307896     0.409248     0.248502    0.182826      0.366474     0.490438     0.555791     0.306616     0.36726      0.566034     0.549495      0.424631\n",
              "[10040 rows x 100 columns]\n"
            ]
          },
          "execution_count": 500,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hidden_layers = best_dl_model.deepfeatures(train_h2o, layer=0)\n",
        "hidden_layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "uOt_dIWzYjGj",
        "outputId": "2224f1b5-1ec2-4a83-c360-55d0a20b5fb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "deepfeatures progress: |█████████████████████████████████████████████████████████| (done) 100%\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table class='dataframe'>\n",
              "<thead>\n",
              "<tr><th style=\"text-align: right;\">  DF.L1.C1</th><th style=\"text-align: right;\">  DF.L1.C2</th><th style=\"text-align: right;\">  DF.L1.C3</th><th style=\"text-align: right;\">  DF.L1.C4</th><th style=\"text-align: right;\">  DF.L1.C5</th><th style=\"text-align: right;\">  DF.L1.C6</th><th style=\"text-align: right;\">  DF.L1.C7</th><th style=\"text-align: right;\">  DF.L1.C8</th><th style=\"text-align: right;\">  DF.L1.C9</th><th style=\"text-align: right;\">  DF.L1.C10</th><th style=\"text-align: right;\">  DF.L1.C11</th><th style=\"text-align: right;\">  DF.L1.C12</th><th style=\"text-align: right;\">  DF.L1.C13</th><th style=\"text-align: right;\">  DF.L1.C14</th><th style=\"text-align: right;\">  DF.L1.C15</th><th style=\"text-align: right;\">  DF.L1.C16</th><th style=\"text-align: right;\">  DF.L1.C17</th><th style=\"text-align: right;\">  DF.L1.C18</th><th style=\"text-align: right;\">  DF.L1.C19</th><th style=\"text-align: right;\">  DF.L1.C20</th><th style=\"text-align: right;\">  DF.L1.C21</th><th style=\"text-align: right;\">  DF.L1.C22</th><th style=\"text-align: right;\">  DF.L1.C23</th><th style=\"text-align: right;\">  DF.L1.C24</th><th style=\"text-align: right;\">  DF.L1.C25</th><th style=\"text-align: right;\">  DF.L1.C26</th><th style=\"text-align: right;\">  DF.L1.C27</th><th style=\"text-align: right;\">  DF.L1.C28</th><th style=\"text-align: right;\">  DF.L1.C29</th><th style=\"text-align: right;\">  DF.L1.C30</th><th style=\"text-align: right;\">  DF.L1.C31</th><th style=\"text-align: right;\">  DF.L1.C32</th><th style=\"text-align: right;\">  DF.L1.C33</th><th style=\"text-align: right;\">  DF.L1.C34</th><th style=\"text-align: right;\">  DF.L1.C35</th><th style=\"text-align: right;\">  DF.L1.C36</th><th style=\"text-align: right;\">  DF.L1.C37</th><th style=\"text-align: right;\">  DF.L1.C38</th><th style=\"text-align: right;\">  DF.L1.C39</th><th style=\"text-align: right;\">  DF.L1.C40</th><th style=\"text-align: right;\">  DF.L1.C41</th><th style=\"text-align: right;\">  DF.L1.C42</th><th style=\"text-align: right;\">  DF.L1.C43</th><th style=\"text-align: right;\">  DF.L1.C44</th><th style=\"text-align: right;\">  DF.L1.C45</th><th style=\"text-align: right;\">  DF.L1.C46</th><th style=\"text-align: right;\">  DF.L1.C47</th><th style=\"text-align: right;\">  DF.L1.C48</th><th style=\"text-align: right;\">  DF.L1.C49</th><th style=\"text-align: right;\">  DF.L1.C50</th><th style=\"text-align: right;\">  DF.L1.C51</th><th style=\"text-align: right;\">  DF.L1.C52</th><th style=\"text-align: right;\">  DF.L1.C53</th><th style=\"text-align: right;\">  DF.L1.C54</th><th style=\"text-align: right;\">  DF.L1.C55</th><th style=\"text-align: right;\">  DF.L1.C56</th><th style=\"text-align: right;\">  DF.L1.C57</th><th style=\"text-align: right;\">  DF.L1.C58</th><th style=\"text-align: right;\">  DF.L1.C59</th><th style=\"text-align: right;\">  DF.L1.C60</th><th style=\"text-align: right;\">  DF.L1.C61</th><th style=\"text-align: right;\">  DF.L1.C62</th><th style=\"text-align: right;\">  DF.L1.C63</th><th style=\"text-align: right;\">  DF.L1.C64</th><th style=\"text-align: right;\">  DF.L1.C65</th><th style=\"text-align: right;\">  DF.L1.C66</th><th style=\"text-align: right;\">  DF.L1.C67</th><th style=\"text-align: right;\">  DF.L1.C68</th><th style=\"text-align: right;\">  DF.L1.C69</th><th style=\"text-align: right;\">  DF.L1.C70</th><th style=\"text-align: right;\">  DF.L1.C71</th><th style=\"text-align: right;\">  DF.L1.C72</th><th style=\"text-align: right;\">  DF.L1.C73</th><th style=\"text-align: right;\">  DF.L1.C74</th><th style=\"text-align: right;\">  DF.L1.C75</th><th style=\"text-align: right;\">  DF.L1.C76</th><th style=\"text-align: right;\">  DF.L1.C77</th><th style=\"text-align: right;\">  DF.L1.C78</th><th style=\"text-align: right;\">  DF.L1.C79</th><th style=\"text-align: right;\">  DF.L1.C80</th><th style=\"text-align: right;\">  DF.L1.C81</th><th style=\"text-align: right;\">  DF.L1.C82</th><th style=\"text-align: right;\">  DF.L1.C83</th><th style=\"text-align: right;\">  DF.L1.C84</th><th style=\"text-align: right;\">  DF.L1.C85</th><th style=\"text-align: right;\">  DF.L1.C86</th><th style=\"text-align: right;\">  DF.L1.C87</th><th style=\"text-align: right;\">  DF.L1.C88</th><th style=\"text-align: right;\">  DF.L1.C89</th><th style=\"text-align: right;\">  DF.L1.C90</th><th style=\"text-align: right;\">  DF.L1.C91</th><th style=\"text-align: right;\">  DF.L1.C92</th><th style=\"text-align: right;\">  DF.L1.C93</th><th style=\"text-align: right;\">  DF.L1.C94</th><th style=\"text-align: right;\">  DF.L1.C95</th><th style=\"text-align: right;\">  DF.L1.C96</th><th style=\"text-align: right;\">  DF.L1.C97</th><th style=\"text-align: right;\">  DF.L1.C98</th><th style=\"text-align: right;\">  DF.L1.C99</th><th style=\"text-align: right;\">  DF.L1.C100</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td style=\"text-align: right;\">  0.476524</td><td style=\"text-align: right;\">  0.351325</td><td style=\"text-align: right;\">  0.43057 </td><td style=\"text-align: right;\">  0.58709 </td><td style=\"text-align: right;\">  0.498779</td><td style=\"text-align: right;\">  0.351123</td><td style=\"text-align: right;\">  0.43165 </td><td style=\"text-align: right;\">  0.434156</td><td style=\"text-align: right;\">  0.513435</td><td style=\"text-align: right;\">   0.425088</td><td style=\"text-align: right;\">   0.645276</td><td style=\"text-align: right;\">   0.59409 </td><td style=\"text-align: right;\">   0.612404</td><td style=\"text-align: right;\">   0.492538</td><td style=\"text-align: right;\">   0.455857</td><td style=\"text-align: right;\">   0.535056</td><td style=\"text-align: right;\">   0.386099</td><td style=\"text-align: right;\">   0.551709</td><td style=\"text-align: right;\">   0.446371</td><td style=\"text-align: right;\">   0.441731</td><td style=\"text-align: right;\">   0.58972 </td><td style=\"text-align: right;\">   0.486671</td><td style=\"text-align: right;\">   0.40743 </td><td style=\"text-align: right;\">   0.471261</td><td style=\"text-align: right;\">   0.520296</td><td style=\"text-align: right;\">   0.603224</td><td style=\"text-align: right;\">   0.518462</td><td style=\"text-align: right;\">   0.596465</td><td style=\"text-align: right;\">   0.370875</td><td style=\"text-align: right;\">   0.557034</td><td style=\"text-align: right;\">   0.540086</td><td style=\"text-align: right;\">   0.46255 </td><td style=\"text-align: right;\">   0.460741</td><td style=\"text-align: right;\">   0.573207</td><td style=\"text-align: right;\">   0.539062</td><td style=\"text-align: right;\">   0.360963</td><td style=\"text-align: right;\">   0.434825</td><td style=\"text-align: right;\">   0.559179</td><td style=\"text-align: right;\">   0.57408 </td><td style=\"text-align: right;\">   0.5832  </td><td style=\"text-align: right;\">   0.479673</td><td style=\"text-align: right;\">   0.425198</td><td style=\"text-align: right;\">   0.529588</td><td style=\"text-align: right;\">   0.541111</td><td style=\"text-align: right;\">   0.524724</td><td style=\"text-align: right;\">   0.492005</td><td style=\"text-align: right;\">   0.556367</td><td style=\"text-align: right;\">   0.340544</td><td style=\"text-align: right;\">   0.43685 </td><td style=\"text-align: right;\">   0.396498</td><td style=\"text-align: right;\">   0.535787</td><td style=\"text-align: right;\">   0.542237</td><td style=\"text-align: right;\">   0.48506 </td><td style=\"text-align: right;\">   0.366534</td><td style=\"text-align: right;\">   0.416451</td><td style=\"text-align: right;\">   0.584673</td><td style=\"text-align: right;\">   0.466162</td><td style=\"text-align: right;\">   0.63709 </td><td style=\"text-align: right;\">   0.408258</td><td style=\"text-align: right;\">   0.421337</td><td style=\"text-align: right;\">  0.581714 </td><td style=\"text-align: right;\">   0.562318</td><td style=\"text-align: right;\">   0.572821</td><td style=\"text-align: right;\">   0.429933</td><td style=\"text-align: right;\">  0.329379 </td><td style=\"text-align: right;\">   0.490875</td><td style=\"text-align: right;\">   0.604129</td><td style=\"text-align: right;\">   0.51733 </td><td style=\"text-align: right;\">   0.553564</td><td style=\"text-align: right;\">   0.491644</td><td style=\"text-align: right;\">   0.520014</td><td style=\"text-align: right;\">   0.425857</td><td style=\"text-align: right;\">   0.5132  </td><td style=\"text-align: right;\">   0.557871</td><td style=\"text-align: right;\">   0.554447</td><td style=\"text-align: right;\">   0.493894</td><td style=\"text-align: right;\">   0.612963</td><td style=\"text-align: right;\">   0.506876</td><td style=\"text-align: right;\">   0.522889</td><td style=\"text-align: right;\">   0.384257</td><td style=\"text-align: right;\">   0.453536</td><td style=\"text-align: right;\">   0.620639</td><td style=\"text-align: right;\">   0.416671</td><td style=\"text-align: right;\">   0.582229</td><td style=\"text-align: right;\">   0.476221</td><td style=\"text-align: right;\">   0.488743</td><td style=\"text-align: right;\">   0.507247</td><td style=\"text-align: right;\">   0.437465</td><td style=\"text-align: right;\">   0.398904</td><td style=\"text-align: right;\">   0.434922</td><td style=\"text-align: right;\">   0.377032</td><td style=\"text-align: right;\">   0.333891</td><td style=\"text-align: right;\">   0.453917</td><td style=\"text-align: right;\">   0.521924</td><td style=\"text-align: right;\">   0.515675</td><td style=\"text-align: right;\">   0.411477</td><td style=\"text-align: right;\">   0.402156</td><td style=\"text-align: right;\">   0.518723</td><td style=\"text-align: right;\">   0.532911</td><td style=\"text-align: right;\">    0.436783</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.483533</td><td style=\"text-align: right;\">  0.3507  </td><td style=\"text-align: right;\">  0.454352</td><td style=\"text-align: right;\">  0.582611</td><td style=\"text-align: right;\">  0.511074</td><td style=\"text-align: right;\">  0.344463</td><td style=\"text-align: right;\">  0.444425</td><td style=\"text-align: right;\">  0.46231 </td><td style=\"text-align: right;\">  0.533681</td><td style=\"text-align: right;\">   0.425043</td><td style=\"text-align: right;\">   0.651939</td><td style=\"text-align: right;\">   0.577442</td><td style=\"text-align: right;\">   0.630776</td><td style=\"text-align: right;\">   0.492383</td><td style=\"text-align: right;\">   0.457919</td><td style=\"text-align: right;\">   0.512822</td><td style=\"text-align: right;\">   0.39109 </td><td style=\"text-align: right;\">   0.556961</td><td style=\"text-align: right;\">   0.449402</td><td style=\"text-align: right;\">   0.446064</td><td style=\"text-align: right;\">   0.604305</td><td style=\"text-align: right;\">   0.482965</td><td style=\"text-align: right;\">   0.402923</td><td style=\"text-align: right;\">   0.468907</td><td style=\"text-align: right;\">   0.510522</td><td style=\"text-align: right;\">   0.605817</td><td style=\"text-align: right;\">   0.518786</td><td style=\"text-align: right;\">   0.609786</td><td style=\"text-align: right;\">   0.358386</td><td style=\"text-align: right;\">   0.577154</td><td style=\"text-align: right;\">   0.563524</td><td style=\"text-align: right;\">   0.473291</td><td style=\"text-align: right;\">   0.452516</td><td style=\"text-align: right;\">   0.579641</td><td style=\"text-align: right;\">   0.540821</td><td style=\"text-align: right;\">   0.35939 </td><td style=\"text-align: right;\">   0.436822</td><td style=\"text-align: right;\">   0.535933</td><td style=\"text-align: right;\">   0.572102</td><td style=\"text-align: right;\">   0.569801</td><td style=\"text-align: right;\">   0.495411</td><td style=\"text-align: right;\">   0.430576</td><td style=\"text-align: right;\">   0.536577</td><td style=\"text-align: right;\">   0.535303</td><td style=\"text-align: right;\">   0.517971</td><td style=\"text-align: right;\">   0.528659</td><td style=\"text-align: right;\">   0.544529</td><td style=\"text-align: right;\">   0.332039</td><td style=\"text-align: right;\">   0.418304</td><td style=\"text-align: right;\">   0.38647 </td><td style=\"text-align: right;\">   0.54159 </td><td style=\"text-align: right;\">   0.53495 </td><td style=\"text-align: right;\">   0.484054</td><td style=\"text-align: right;\">   0.367111</td><td style=\"text-align: right;\">   0.440062</td><td style=\"text-align: right;\">   0.574687</td><td style=\"text-align: right;\">   0.467009</td><td style=\"text-align: right;\">   0.624428</td><td style=\"text-align: right;\">   0.398106</td><td style=\"text-align: right;\">   0.405138</td><td style=\"text-align: right;\">  0.597738 </td><td style=\"text-align: right;\">   0.587179</td><td style=\"text-align: right;\">   0.596047</td><td style=\"text-align: right;\">   0.447533</td><td style=\"text-align: right;\">  0.321964 </td><td style=\"text-align: right;\">   0.49241 </td><td style=\"text-align: right;\">   0.591987</td><td style=\"text-align: right;\">   0.524206</td><td style=\"text-align: right;\">   0.541211</td><td style=\"text-align: right;\">   0.513142</td><td style=\"text-align: right;\">   0.525888</td><td style=\"text-align: right;\">   0.41922 </td><td style=\"text-align: right;\">   0.49343 </td><td style=\"text-align: right;\">   0.543739</td><td style=\"text-align: right;\">   0.564217</td><td style=\"text-align: right;\">   0.509384</td><td style=\"text-align: right;\">   0.612905</td><td style=\"text-align: right;\">   0.495906</td><td style=\"text-align: right;\">   0.534107</td><td style=\"text-align: right;\">   0.377825</td><td style=\"text-align: right;\">   0.449545</td><td style=\"text-align: right;\">   0.648613</td><td style=\"text-align: right;\">   0.402344</td><td style=\"text-align: right;\">   0.568061</td><td style=\"text-align: right;\">   0.489448</td><td style=\"text-align: right;\">   0.496701</td><td style=\"text-align: right;\">   0.509203</td><td style=\"text-align: right;\">   0.455851</td><td style=\"text-align: right;\">   0.396456</td><td style=\"text-align: right;\">   0.445851</td><td style=\"text-align: right;\">   0.366975</td><td style=\"text-align: right;\">   0.330574</td><td style=\"text-align: right;\">   0.43947 </td><td style=\"text-align: right;\">   0.510942</td><td style=\"text-align: right;\">   0.526586</td><td style=\"text-align: right;\">   0.405236</td><td style=\"text-align: right;\">   0.417277</td><td style=\"text-align: right;\">   0.523227</td><td style=\"text-align: right;\">   0.524733</td><td style=\"text-align: right;\">    0.447753</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.476719</td><td style=\"text-align: right;\">  0.21866 </td><td style=\"text-align: right;\">  0.448906</td><td style=\"text-align: right;\">  0.644451</td><td style=\"text-align: right;\">  0.536158</td><td style=\"text-align: right;\">  0.193744</td><td style=\"text-align: right;\">  0.408292</td><td style=\"text-align: right;\">  0.4617  </td><td style=\"text-align: right;\">  0.584611</td><td style=\"text-align: right;\">   0.352388</td><td style=\"text-align: right;\">   0.82812 </td><td style=\"text-align: right;\">   0.629441</td><td style=\"text-align: right;\">   0.765838</td><td style=\"text-align: right;\">   0.491766</td><td style=\"text-align: right;\">   0.429619</td><td style=\"text-align: right;\">   0.485566</td><td style=\"text-align: right;\">   0.301043</td><td style=\"text-align: right;\">   0.592637</td><td style=\"text-align: right;\">   0.416271</td><td style=\"text-align: right;\">   0.395089</td><td style=\"text-align: right;\">   0.736464</td><td style=\"text-align: right;\">   0.449477</td><td style=\"text-align: right;\">   0.317513</td><td style=\"text-align: right;\">   0.452399</td><td style=\"text-align: right;\">   0.496157</td><td style=\"text-align: right;\">   0.681924</td><td style=\"text-align: right;\">   0.517563</td><td style=\"text-align: right;\">   0.709129</td><td style=\"text-align: right;\">   0.217015</td><td style=\"text-align: right;\">   0.683404</td><td style=\"text-align: right;\">   0.656651</td><td style=\"text-align: right;\">   0.46969 </td><td style=\"text-align: right;\">   0.40631 </td><td style=\"text-align: right;\">   0.668587</td><td style=\"text-align: right;\">   0.586635</td><td style=\"text-align: right;\">   0.225311</td><td style=\"text-align: right;\">   0.370952</td><td style=\"text-align: right;\">   0.528265</td><td style=\"text-align: right;\">   0.636284</td><td style=\"text-align: right;\">   0.622974</td><td style=\"text-align: right;\">   0.501973</td><td style=\"text-align: right;\">   0.381498</td><td style=\"text-align: right;\">   0.580211</td><td style=\"text-align: right;\">   0.566041</td><td style=\"text-align: right;\">   0.537395</td><td style=\"text-align: right;\">   0.6135  </td><td style=\"text-align: right;\">   0.550915</td><td style=\"text-align: right;\">   0.171354</td><td style=\"text-align: right;\">   0.333916</td><td style=\"text-align: right;\">   0.267827</td><td style=\"text-align: right;\">   0.594519</td><td style=\"text-align: right;\">   0.560313</td><td style=\"text-align: right;\">   0.484369</td><td style=\"text-align: right;\">   0.242651</td><td style=\"text-align: right;\">   0.416799</td><td style=\"text-align: right;\">   0.628414</td><td style=\"text-align: right;\">   0.425928</td><td style=\"text-align: right;\">   0.693353</td><td style=\"text-align: right;\">   0.3055  </td><td style=\"text-align: right;\">   0.319055</td><td style=\"text-align: right;\">  0.696461 </td><td style=\"text-align: right;\">   0.710957</td><td style=\"text-align: right;\">   0.709285</td><td style=\"text-align: right;\">   0.430433</td><td style=\"text-align: right;\">  0.141121 </td><td style=\"text-align: right;\">   0.479015</td><td style=\"text-align: right;\">   0.637172</td><td style=\"text-align: right;\">   0.557832</td><td style=\"text-align: right;\">   0.567066</td><td style=\"text-align: right;\">   0.541517</td><td style=\"text-align: right;\">   0.548413</td><td style=\"text-align: right;\">   0.321015</td><td style=\"text-align: right;\">   0.453319</td><td style=\"text-align: right;\">   0.545561</td><td style=\"text-align: right;\">   0.62308 </td><td style=\"text-align: right;\">   0.55223 </td><td style=\"text-align: right;\">   0.692383</td><td style=\"text-align: right;\">   0.485987</td><td style=\"text-align: right;\">   0.575138</td><td style=\"text-align: right;\">   0.273708</td><td style=\"text-align: right;\">   0.397425</td><td style=\"text-align: right;\">   0.812026</td><td style=\"text-align: right;\">   0.290709</td><td style=\"text-align: right;\">   0.608949</td><td style=\"text-align: right;\">   0.498399</td><td style=\"text-align: right;\">   0.499972</td><td style=\"text-align: right;\">   0.516034</td><td style=\"text-align: right;\">   0.445563</td><td style=\"text-align: right;\">   0.3101  </td><td style=\"text-align: right;\">   0.410384</td><td style=\"text-align: right;\">   0.251289</td><td style=\"text-align: right;\">   0.186443</td><td style=\"text-align: right;\">   0.367814</td><td style=\"text-align: right;\">   0.490345</td><td style=\"text-align: right;\">   0.555209</td><td style=\"text-align: right;\">   0.30878 </td><td style=\"text-align: right;\">   0.369205</td><td style=\"text-align: right;\">   0.565621</td><td style=\"text-align: right;\">   0.548981</td><td style=\"text-align: right;\">    0.425608</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.475652</td><td style=\"text-align: right;\">  0.471325</td><td style=\"text-align: right;\">  0.411169</td><td style=\"text-align: right;\">  0.535027</td><td style=\"text-align: right;\">  0.463605</td><td style=\"text-align: right;\">  0.493546</td><td style=\"text-align: right;\">  0.450426</td><td style=\"text-align: right;\">  0.404383</td><td style=\"text-align: right;\">  0.445556</td><td style=\"text-align: right;\">   0.489934</td><td style=\"text-align: right;\">   0.480707</td><td style=\"text-align: right;\">   0.564829</td><td style=\"text-align: right;\">   0.472623</td><td style=\"text-align: right;\">   0.494889</td><td style=\"text-align: right;\">   0.479216</td><td style=\"text-align: right;\">   0.582933</td><td style=\"text-align: right;\">   0.46211 </td><td style=\"text-align: right;\">   0.511714</td><td style=\"text-align: right;\">   0.474626</td><td style=\"text-align: right;\">   0.481242</td><td style=\"text-align: right;\">   0.457599</td><td style=\"text-align: right;\">   0.518279</td><td style=\"text-align: right;\">   0.490551</td><td style=\"text-align: right;\">   0.489997</td><td style=\"text-align: right;\">   0.542503</td><td style=\"text-align: right;\">   0.530977</td><td style=\"text-align: right;\">   0.51634 </td><td style=\"text-align: right;\">   0.492904</td><td style=\"text-align: right;\">   0.510573</td><td style=\"text-align: right;\">   0.441324</td><td style=\"text-align: right;\">   0.431688</td><td style=\"text-align: right;\">   0.454377</td><td style=\"text-align: right;\">   0.512683</td><td style=\"text-align: right;\">   0.486849</td><td style=\"text-align: right;\">   0.497362</td><td style=\"text-align: right;\">   0.482481</td><td style=\"text-align: right;\">   0.490539</td><td style=\"text-align: right;\">   0.591165</td><td style=\"text-align: right;\">   0.520454</td><td style=\"text-align: right;\">   0.550119</td><td style=\"text-align: right;\">   0.455922</td><td style=\"text-align: right;\">   0.464777</td><td style=\"text-align: right;\">   0.482622</td><td style=\"text-align: right;\">   0.521771</td><td style=\"text-align: right;\">   0.516639</td><td style=\"text-align: right;\">   0.377338</td><td style=\"text-align: right;\">   0.561814</td><td style=\"text-align: right;\">   0.494307</td><td style=\"text-align: right;\">   0.533745</td><td style=\"text-align: right;\">   0.51242 </td><td style=\"text-align: right;\">   0.483746</td><td style=\"text-align: right;\">   0.529009</td><td style=\"text-align: right;\">   0.487671</td><td style=\"text-align: right;\">   0.477103</td><td style=\"text-align: right;\">   0.412336</td><td style=\"text-align: right;\">   0.546124</td><td style=\"text-align: right;\">   0.50082 </td><td style=\"text-align: right;\">   0.588356</td><td style=\"text-align: right;\">   0.504501</td><td style=\"text-align: right;\">   0.518185</td><td style=\"text-align: right;\">  0.475808 </td><td style=\"text-align: right;\">   0.426361</td><td style=\"text-align: right;\">   0.44599 </td><td style=\"text-align: right;\">   0.427835</td><td style=\"text-align: right;\">  0.498145 </td><td style=\"text-align: right;\">   0.499352</td><td style=\"text-align: right;\">   0.575671</td><td style=\"text-align: right;\">   0.479623</td><td style=\"text-align: right;\">   0.544211</td><td style=\"text-align: right;\">   0.441967</td><td style=\"text-align: right;\">   0.491978</td><td style=\"text-align: right;\">   0.51906 </td><td style=\"text-align: right;\">   0.569662</td><td style=\"text-align: right;\">   0.569884</td><td style=\"text-align: right;\">   0.490172</td><td style=\"text-align: right;\">   0.44115 </td><td style=\"text-align: right;\">   0.540606</td><td style=\"text-align: right;\">   0.528582</td><td style=\"text-align: right;\">   0.474093</td><td style=\"text-align: right;\">   0.48585 </td><td style=\"text-align: right;\">   0.503699</td><td style=\"text-align: right;\">   0.441739</td><td style=\"text-align: right;\">   0.532001</td><td style=\"text-align: right;\">   0.561942</td><td style=\"text-align: right;\">   0.453403</td><td style=\"text-align: right;\">   0.47603 </td><td style=\"text-align: right;\">   0.498869</td><td style=\"text-align: right;\">   0.428154</td><td style=\"text-align: right;\">   0.479571</td><td style=\"text-align: right;\">   0.455368</td><td style=\"text-align: right;\">   0.493249</td><td style=\"text-align: right;\">   0.467596</td><td style=\"text-align: right;\">   0.534564</td><td style=\"text-align: right;\">   0.550453</td><td style=\"text-align: right;\">   0.477501</td><td style=\"text-align: right;\">   0.50472 </td><td style=\"text-align: right;\">   0.430349</td><td style=\"text-align: right;\">   0.47841 </td><td style=\"text-align: right;\">   0.522615</td><td style=\"text-align: right;\">    0.445604</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.432472</td><td style=\"text-align: right;\">  0.844933</td><td style=\"text-align: right;\">  0.224797</td><td style=\"text-align: right;\">  0.40672 </td><td style=\"text-align: right;\">  0.288238</td><td style=\"text-align: right;\">  0.975345</td><td style=\"text-align: right;\">  0.451478</td><td style=\"text-align: right;\">  0.181005</td><td style=\"text-align: right;\">  0.144282</td><td style=\"text-align: right;\">   0.70257 </td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">   0.551071</td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">   0.482316</td><td style=\"text-align: right;\">   0.541771</td><td style=\"text-align: right;\">   0.842303</td><td style=\"text-align: right;\">   0.675488</td><td style=\"text-align: right;\">   0.386859</td><td style=\"text-align: right;\">   0.530074</td><td style=\"text-align: right;\">   0.606927</td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">   0.666616</td><td style=\"text-align: right;\">   0.754536</td><td style=\"text-align: right;\">   0.543751</td><td style=\"text-align: right;\">   0.671961</td><td style=\"text-align: right;\">   0.304288</td><td style=\"text-align: right;\">   0.544051</td><td style=\"text-align: right;\">   0.105269</td><td style=\"text-align: right;\">   1.0174  </td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">   0.377443</td><td style=\"text-align: right;\">   0.693907</td><td style=\"text-align: right;\">   0.17901 </td><td style=\"text-align: right;\">   0.339147</td><td style=\"text-align: right;\">   0.881968</td><td style=\"text-align: right;\">   0.675098</td><td style=\"text-align: right;\">   0.795846</td><td style=\"text-align: right;\">   0.336392</td><td style=\"text-align: right;\">   0.505437</td><td style=\"text-align: right;\">   0.319754</td><td style=\"text-align: right;\">   0.551649</td><td style=\"text-align: right;\">   0.305513</td><td style=\"text-align: right;\">   0.461062</td><td style=\"text-align: right;\">   0.495911</td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">   0.650957</td><td style=\"text-align: right;\">   1.01639 </td><td style=\"text-align: right;\">   0.911347</td><td style=\"text-align: right;\">   0.940919</td><td style=\"text-align: right;\">   0.271759</td><td style=\"text-align: right;\">   0.500085</td><td style=\"text-align: right;\">   0.477703</td><td style=\"text-align: right;\">   0.829501</td><td style=\"text-align: right;\">   0.286115</td><td style=\"text-align: right;\">   0.483716</td><td style=\"text-align: right;\">   0.622747</td><td style=\"text-align: right;\">   0.495894</td><td style=\"text-align: right;\">   0.830116</td><td style=\"text-align: right;\">   0.872854</td><td style=\"text-align: right;\">  0.0678372</td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">   0.323815</td><td style=\"text-align: right;\">  1.08544  </td><td style=\"text-align: right;\">   0.543945</td><td style=\"text-align: right;\">   0.554291</td><td style=\"text-align: right;\">   0.3297  </td><td style=\"text-align: right;\">   0.564405</td><td style=\"text-align: right;\">   0.198609</td><td style=\"text-align: right;\">   0.391935</td><td style=\"text-align: right;\">   0.868632</td><td style=\"text-align: right;\">   0.847124</td><td style=\"text-align: right;\">   0.691131</td><td style=\"text-align: right;\">   0.25159 </td><td style=\"text-align: right;\">   0.174414</td><td style=\"text-align: right;\">   0.324157</td><td style=\"text-align: right;\">   0.636397</td><td style=\"text-align: right;\">   0.265265</td><td style=\"text-align: right;\">   0.823606</td><td style=\"text-align: right;\">   0.690035</td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">   0.965746</td><td style=\"text-align: right;\">   0.547384</td><td style=\"text-align: right;\">   0.328718</td><td style=\"text-align: right;\">   0.412005</td><td style=\"text-align: right;\">   0.467289</td><td style=\"text-align: right;\">   0.302015</td><td style=\"text-align: right;\">   0.741138</td><td style=\"text-align: right;\">   0.464123</td><td style=\"text-align: right;\">   0.889139</td><td style=\"text-align: right;\">   0.898089</td><td style=\"text-align: right;\">   0.845722</td><td style=\"text-align: right;\">   0.712538</td><td style=\"text-align: right;\">   0.31423 </td><td style=\"text-align: right;\">   0.829344</td><td style=\"text-align: right;\">   0.435924</td><td style=\"text-align: right;\">   0.300186</td><td style=\"text-align: right;\">   0.494526</td><td style=\"text-align: right;\">    0.414143</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.55441 </td><td style=\"text-align: right;\">  0.567208</td><td style=\"text-align: right;\">  0.658179</td><td style=\"text-align: right;\">  0.428425</td><td style=\"text-align: right;\">  0.570888</td><td style=\"text-align: right;\">  0.531078</td><td style=\"text-align: right;\">  0.59493 </td><td style=\"text-align: right;\">  0.665731</td><td style=\"text-align: right;\">  0.593789</td><td style=\"text-align: right;\">   0.531005</td><td style=\"text-align: right;\">   0.438878</td><td style=\"text-align: right;\">   0.366115</td><td style=\"text-align: right;\">   0.575121</td><td style=\"text-align: right;\">   0.521817</td><td style=\"text-align: right;\">   0.520181</td><td style=\"text-align: right;\">   0.385514</td><td style=\"text-align: right;\">   0.578406</td><td style=\"text-align: right;\">   0.496011</td><td style=\"text-align: right;\">   0.555911</td><td style=\"text-align: right;\">   0.528229</td><td style=\"text-align: right;\">   0.54356 </td><td style=\"text-align: right;\">   0.464297</td><td style=\"text-align: right;\">   0.53301 </td><td style=\"text-align: right;\">   0.503185</td><td style=\"text-align: right;\">   0.440284</td><td style=\"text-align: right;\">   0.484838</td><td style=\"text-align: right;\">   0.470747</td><td style=\"text-align: right;\">   0.549994</td><td style=\"text-align: right;\">   0.477135</td><td style=\"text-align: right;\">   0.580652</td><td style=\"text-align: right;\">   0.594821</td><td style=\"text-align: right;\">   0.559173</td><td style=\"text-align: right;\">   0.494552</td><td style=\"text-align: right;\">   0.494641</td><td style=\"text-align: right;\">   0.505499</td><td style=\"text-align: right;\">   0.548014</td><td style=\"text-align: right;\">   0.531433</td><td style=\"text-align: right;\">   0.381313</td><td style=\"text-align: right;\">   0.491431</td><td style=\"text-align: right;\">   0.394764</td><td style=\"text-align: right;\">   0.581734</td><td style=\"text-align: right;\">   0.570582</td><td style=\"text-align: right;\">   0.513815</td><td style=\"text-align: right;\">   0.481602</td><td style=\"text-align: right;\">   0.476893</td><td style=\"text-align: right;\">   0.677937</td><td style=\"text-align: right;\">   0.420255</td><td style=\"text-align: right;\">   0.528612</td><td style=\"text-align: right;\">   0.438105</td><td style=\"text-align: right;\">   0.474026</td><td style=\"text-align: right;\">   0.53229 </td><td style=\"text-align: right;\">   0.473961</td><td style=\"text-align: right;\">   0.509224</td><td style=\"text-align: right;\">   0.555417</td><td style=\"text-align: right;\">   0.654181</td><td style=\"text-align: right;\">   0.399499</td><td style=\"text-align: right;\">   0.513376</td><td style=\"text-align: right;\">   0.415937</td><td style=\"text-align: right;\">   0.509874</td><td style=\"text-align: right;\">   0.461929</td><td style=\"text-align: right;\">  0.558411 </td><td style=\"text-align: right;\">   0.608461</td><td style=\"text-align: right;\">   0.583392</td><td style=\"text-align: right;\">   0.628866</td><td style=\"text-align: right;\">  0.522576 </td><td style=\"text-align: right;\">   0.490189</td><td style=\"text-align: right;\">   0.416643</td><td style=\"text-align: right;\">   0.518904</td><td style=\"text-align: right;\">   0.422044</td><td style=\"text-align: right;\">   0.604086</td><td style=\"text-align: right;\">   0.504977</td><td style=\"text-align: right;\">   0.485992</td><td style=\"text-align: right;\">   0.398004</td><td style=\"text-align: right;\">   0.410326</td><td style=\"text-align: right;\">   0.525418</td><td style=\"text-align: right;\">   0.597925</td><td style=\"text-align: right;\">   0.468515</td><td style=\"text-align: right;\">   0.450821</td><td style=\"text-align: right;\">   0.552463</td><td style=\"text-align: right;\">   0.516592</td><td style=\"text-align: right;\">   0.484799</td><td style=\"text-align: right;\">   0.5566  </td><td style=\"text-align: right;\">   0.467741</td><td style=\"text-align: right;\">   0.423157</td><td style=\"text-align: right;\">   0.561207</td><td style=\"text-align: right;\">   0.526199</td><td style=\"text-align: right;\">   0.510647</td><td style=\"text-align: right;\">   0.627917</td><td style=\"text-align: right;\">   0.523427</td><td style=\"text-align: right;\">   0.586414</td><td style=\"text-align: right;\">   0.50365 </td><td style=\"text-align: right;\">   0.545875</td><td style=\"text-align: right;\">   0.461171</td><td style=\"text-align: right;\">   0.431335</td><td style=\"text-align: right;\">   0.546656</td><td style=\"text-align: right;\">   0.509809</td><td style=\"text-align: right;\">   0.625828</td><td style=\"text-align: right;\">   0.537315</td><td style=\"text-align: right;\">   0.476626</td><td style=\"text-align: right;\">    0.575106</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.481561</td><td style=\"text-align: right;\">  0.174852</td><td style=\"text-align: right;\">  0.470563</td><td style=\"text-align: right;\">  0.659853</td><td style=\"text-align: right;\">  0.556553</td><td style=\"text-align: right;\">  0.137548</td><td style=\"text-align: right;\">  0.408519</td><td style=\"text-align: right;\">  0.488361</td><td style=\"text-align: right;\">  0.620441</td><td style=\"text-align: right;\">   0.327971</td><td style=\"text-align: right;\">   0.893121</td><td style=\"text-align: right;\">   0.630645</td><td style=\"text-align: right;\">   0.828683</td><td style=\"text-align: right;\">   0.49242 </td><td style=\"text-align: right;\">   0.422323</td><td style=\"text-align: right;\">   0.455063</td><td style=\"text-align: right;\">   0.276235</td><td style=\"text-align: right;\">   0.608206</td><td style=\"text-align: right;\">   0.409172</td><td style=\"text-align: right;\">   0.381377</td><td style=\"text-align: right;\">   0.795365</td><td style=\"text-align: right;\">   0.433352</td><td style=\"text-align: right;\">   0.285937</td><td style=\"text-align: right;\">   0.445483</td><td style=\"text-align: right;\">   0.481392</td><td style=\"text-align: right;\">   0.708885</td><td style=\"text-align: right;\">   0.515728</td><td style=\"text-align: right;\">   0.754613</td><td style=\"text-align: right;\">   0.158015</td><td style=\"text-align: right;\">   0.738456</td><td style=\"text-align: right;\">   0.710043</td><td style=\"text-align: right;\">   0.478834</td><td style=\"text-align: right;\">   0.384205</td><td style=\"text-align: right;\">   0.704368</td><td style=\"text-align: right;\">   0.604345</td><td style=\"text-align: right;\">   0.179115</td><td style=\"text-align: right;\">   0.350234</td><td style=\"text-align: right;\">   0.503717</td><td style=\"text-align: right;\">   0.656747</td><td style=\"text-align: right;\">   0.627828</td><td style=\"text-align: right;\">   0.518618</td><td style=\"text-align: right;\">   0.371057</td><td style=\"text-align: right;\">   0.601126</td><td style=\"text-align: right;\">   0.571933</td><td style=\"text-align: right;\">   0.538638</td><td style=\"text-align: right;\">   0.677003</td><td style=\"text-align: right;\">   0.540889</td><td style=\"text-align: right;\">   0.110294</td><td style=\"text-align: right;\">   0.289008</td><td style=\"text-align: right;\">   0.218295</td><td style=\"text-align: right;\">   0.618548</td><td style=\"text-align: right;\">   0.562733</td><td style=\"text-align: right;\">   0.484621</td><td style=\"text-align: right;\">   0.201822</td><td style=\"text-align: right;\">   0.431872</td><td style=\"text-align: right;\">   0.635975</td><td style=\"text-align: right;\">   0.412377</td><td style=\"text-align: right;\">   0.703889</td><td style=\"text-align: right;\">   0.266433</td><td style=\"text-align: right;\">   0.276409</td><td style=\"text-align: right;\">  0.744345 </td><td style=\"text-align: right;\">   0.776637</td><td style=\"text-align: right;\">   0.768747</td><td style=\"text-align: right;\">   0.442368</td><td style=\"text-align: right;\">  0.0733437</td><td style=\"text-align: right;\">   0.474845</td><td style=\"text-align: right;\">   0.63989 </td><td style=\"text-align: right;\">   0.575464</td><td style=\"text-align: right;\">   0.564157</td><td style=\"text-align: right;\">   0.570807</td><td style=\"text-align: right;\">   0.560721</td><td style=\"text-align: right;\">   0.281063</td><td style=\"text-align: right;\">   0.420819</td><td style=\"text-align: right;\">   0.531795</td><td style=\"text-align: right;\">   0.6514  </td><td style=\"text-align: right;\">   0.582558</td><td style=\"text-align: right;\">   0.71801 </td><td style=\"text-align: right;\">   0.472779</td><td style=\"text-align: right;\">   0.599554</td><td style=\"text-align: right;\">   0.233791</td><td style=\"text-align: right;\">   0.375951</td><td style=\"text-align: right;\">   0.891497</td><td style=\"text-align: right;\">   0.239998</td><td style=\"text-align: right;\">   0.609693</td><td style=\"text-align: right;\">   0.513573</td><td style=\"text-align: right;\">   0.508043</td><td style=\"text-align: right;\">   0.519957</td><td style=\"text-align: right;\">   0.460285</td><td style=\"text-align: right;\">   0.279453</td><td style=\"text-align: right;\">   0.409376</td><td style=\"text-align: right;\">   0.204323</td><td style=\"text-align: right;\">   0.136011</td><td style=\"text-align: right;\">   0.33085 </td><td style=\"text-align: right;\">   0.472026</td><td style=\"text-align: right;\">   0.574721</td><td style=\"text-align: right;\">   0.270899</td><td style=\"text-align: right;\">   0.368423</td><td style=\"text-align: right;\">   0.58537 </td><td style=\"text-align: right;\">   0.55079 </td><td style=\"text-align: right;\">    0.429191</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.480565</td><td style=\"text-align: right;\">  0.180871</td><td style=\"text-align: right;\">  0.466495</td><td style=\"text-align: right;\">  0.658026</td><td style=\"text-align: right;\">  0.553186</td><td style=\"text-align: right;\">  0.145602</td><td style=\"text-align: right;\">  0.407966</td><td style=\"text-align: right;\">  0.483548</td><td style=\"text-align: right;\">  0.614698</td><td style=\"text-align: right;\">   0.331387</td><td style=\"text-align: right;\">   0.883778</td><td style=\"text-align: right;\">   0.631198</td><td style=\"text-align: right;\">   0.819128</td><td style=\"text-align: right;\">   0.492189</td><td style=\"text-align: right;\">   0.423235</td><td style=\"text-align: right;\">   0.460224</td><td style=\"text-align: right;\">   0.279415</td><td style=\"text-align: right;\">   0.60606 </td><td style=\"text-align: right;\">   0.409868</td><td style=\"text-align: right;\">   0.383245</td><td style=\"text-align: right;\">   0.786421</td><td style=\"text-align: right;\">   0.435952</td><td style=\"text-align: right;\">   0.290346</td><td style=\"text-align: right;\">   0.446407</td><td style=\"text-align: right;\">   0.483939</td><td style=\"text-align: right;\">   0.70515 </td><td style=\"text-align: right;\">   0.51623 </td><td style=\"text-align: right;\">   0.747783</td><td style=\"text-align: right;\">   0.166748</td><td style=\"text-align: right;\">   0.729903</td><td style=\"text-align: right;\">   0.701673</td><td style=\"text-align: right;\">   0.477125</td><td style=\"text-align: right;\">   0.387452</td><td style=\"text-align: right;\">   0.699117</td><td style=\"text-align: right;\">   0.601707</td><td style=\"text-align: right;\">   0.185623</td><td style=\"text-align: right;\">   0.353134</td><td style=\"text-align: right;\">   0.508044</td><td style=\"text-align: right;\">   0.653823</td><td style=\"text-align: right;\">   0.627664</td><td style=\"text-align: right;\">   0.515769</td><td style=\"text-align: right;\">   0.372162</td><td style=\"text-align: right;\">   0.597974</td><td style=\"text-align: right;\">   0.57117 </td><td style=\"text-align: right;\">   0.538547</td><td style=\"text-align: right;\">   0.666654</td><td style=\"text-align: right;\">   0.542897</td><td style=\"text-align: right;\">   0.119056</td><td style=\"text-align: right;\">   0.29586 </td><td style=\"text-align: right;\">   0.225686</td><td style=\"text-align: right;\">   0.61483 </td><td style=\"text-align: right;\">   0.562532</td><td style=\"text-align: right;\">   0.484463</td><td style=\"text-align: right;\">   0.207509</td><td style=\"text-align: right;\">   0.428805</td><td style=\"text-align: right;\">   0.635421</td><td style=\"text-align: right;\">   0.41434 </td><td style=\"text-align: right;\">   0.702986</td><td style=\"text-align: right;\">   0.272046</td><td style=\"text-align: right;\">   0.282767</td><td style=\"text-align: right;\">  0.737075 </td><td style=\"text-align: right;\">   0.766367</td><td style=\"text-align: right;\">   0.759599</td><td style=\"text-align: right;\">   0.439872</td><td style=\"text-align: right;\">  0.0831707</td><td style=\"text-align: right;\">   0.475529</td><td style=\"text-align: right;\">   0.640081</td><td style=\"text-align: right;\">   0.57276 </td><td style=\"text-align: right;\">   0.565002</td><td style=\"text-align: right;\">   0.565992</td><td style=\"text-align: right;\">   0.558927</td><td style=\"text-align: right;\">   0.287058</td><td style=\"text-align: right;\">   0.426191</td><td style=\"text-align: right;\">   0.53441 </td><td style=\"text-align: right;\">   0.647176</td><td style=\"text-align: right;\">   0.577519</td><td style=\"text-align: right;\">   0.714562</td><td style=\"text-align: right;\">   0.474952</td><td style=\"text-align: right;\">   0.595719</td><td style=\"text-align: right;\">   0.239462</td><td style=\"text-align: right;\">   0.379176</td><td style=\"text-align: right;\">   0.879564</td><td style=\"text-align: right;\">   0.247624</td><td style=\"text-align: right;\">   0.610051</td><td style=\"text-align: right;\">   0.510989</td><td style=\"text-align: right;\">   0.506725</td><td style=\"text-align: right;\">   0.519342</td><td style=\"text-align: right;\">   0.45739 </td><td style=\"text-align: right;\">   0.283746</td><td style=\"text-align: right;\">   0.40905 </td><td style=\"text-align: right;\">   0.211087</td><td style=\"text-align: right;\">   0.143057</td><td style=\"text-align: right;\">   0.336478</td><td style=\"text-align: right;\">   0.475165</td><td style=\"text-align: right;\">   0.571655</td><td style=\"text-align: right;\">   0.276401</td><td style=\"text-align: right;\">   0.367803</td><td style=\"text-align: right;\">   0.582213</td><td style=\"text-align: right;\">   0.550623</td><td style=\"text-align: right;\">    0.428189</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.464251</td><td style=\"text-align: right;\">  0.312435</td><td style=\"text-align: right;\">  0.395287</td><td style=\"text-align: right;\">  0.613949</td><td style=\"text-align: right;\">  0.488761</td><td style=\"text-align: right;\">  0.316773</td><td style=\"text-align: right;\">  0.404748</td><td style=\"text-align: right;\">  0.398033</td><td style=\"text-align: right;\">  0.502955</td><td style=\"text-align: right;\">   0.405451</td><td style=\"text-align: right;\">   0.685048</td><td style=\"text-align: right;\">   0.631566</td><td style=\"text-align: right;\">   0.624262</td><td style=\"text-align: right;\">   0.488451</td><td style=\"text-align: right;\">   0.44471 </td><td style=\"text-align: right;\">   0.557049</td><td style=\"text-align: right;\">   0.352588</td><td style=\"text-align: right;\">   0.561101</td><td style=\"text-align: right;\">   0.428508</td><td style=\"text-align: right;\">   0.425642</td><td style=\"text-align: right;\">   0.603133</td><td style=\"text-align: right;\">   0.488013</td><td style=\"text-align: right;\">   0.384796</td><td style=\"text-align: right;\">   0.466044</td><td style=\"text-align: right;\">   0.531776</td><td style=\"text-align: right;\">   0.62449 </td><td style=\"text-align: right;\">   0.524953</td><td style=\"text-align: right;\">   0.607852</td><td style=\"text-align: right;\">   0.348199</td><td style=\"text-align: right;\">   0.558223</td><td style=\"text-align: right;\">   0.53558 </td><td style=\"text-align: right;\">   0.447393</td><td style=\"text-align: right;\">   0.454031</td><td style=\"text-align: right;\">   0.589294</td><td style=\"text-align: right;\">   0.546586</td><td style=\"text-align: right;\">   0.326067</td><td style=\"text-align: right;\">   0.416653</td><td style=\"text-align: right;\">   0.586565</td><td style=\"text-align: right;\">   0.590216</td><td style=\"text-align: right;\">   0.614882</td><td style=\"text-align: right;\">   0.46375 </td><td style=\"text-align: right;\">   0.400948</td><td style=\"text-align: right;\">   0.533785</td><td style=\"text-align: right;\">   0.552272</td><td style=\"text-align: right;\">   0.533636</td><td style=\"text-align: right;\">   0.466893</td><td style=\"text-align: right;\">   0.577325</td><td style=\"text-align: right;\">   0.304659</td><td style=\"text-align: right;\">   0.43351 </td><td style=\"text-align: right;\">   0.379008</td><td style=\"text-align: right;\">   0.539091</td><td style=\"text-align: right;\">   0.554477</td><td style=\"text-align: right;\">   0.481868</td><td style=\"text-align: right;\">   0.331708</td><td style=\"text-align: right;\">   0.378567</td><td style=\"text-align: right;\">   0.615449</td><td style=\"text-align: right;\">   0.456678</td><td style=\"text-align: right;\">   0.674237</td><td style=\"text-align: right;\">   0.389269</td><td style=\"text-align: right;\">   0.412093</td><td style=\"text-align: right;\">  0.589459 </td><td style=\"text-align: right;\">   0.560927</td><td style=\"text-align: right;\">   0.575851</td><td style=\"text-align: right;\">   0.398719</td><td style=\"text-align: right;\">  0.291233 </td><td style=\"text-align: right;\">   0.489852</td><td style=\"text-align: right;\">   0.634949</td><td style=\"text-align: right;\">   0.518442</td><td style=\"text-align: right;\">   0.575316</td><td style=\"text-align: right;\">   0.474958</td><td style=\"text-align: right;\">   0.522779</td><td style=\"text-align: right;\">   0.411696</td><td style=\"text-align: right;\">   0.529182</td><td style=\"text-align: right;\">   0.580434</td><td style=\"text-align: right;\">   0.56111 </td><td style=\"text-align: right;\">   0.480234</td><td style=\"text-align: right;\">   0.638437</td><td style=\"text-align: right;\">   0.515529</td><td style=\"text-align: right;\">   0.520001</td><td style=\"text-align: right;\">   0.359695</td><td style=\"text-align: right;\">   0.446163</td><td style=\"text-align: right;\">   0.636775</td><td style=\"text-align: right;\">   0.403944</td><td style=\"text-align: right;\">   0.609076</td><td style=\"text-align: right;\">   0.463204</td><td style=\"text-align: right;\">   0.482622</td><td style=\"text-align: right;\">   0.506996</td><td style=\"text-align: right;\">   0.407799</td><td style=\"text-align: right;\">   0.376078</td><td style=\"text-align: right;\">   0.409928</td><td style=\"text-align: right;\">   0.352973</td><td style=\"text-align: right;\">   0.295099</td><td style=\"text-align: right;\">   0.450024</td><td style=\"text-align: right;\">   0.534612</td><td style=\"text-align: right;\">   0.511784</td><td style=\"text-align: right;\">   0.39211 </td><td style=\"text-align: right;\">   0.365739</td><td style=\"text-align: right;\">   0.518444</td><td style=\"text-align: right;\">   0.543532</td><td style=\"text-align: right;\">    0.414541</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.459919</td><td style=\"text-align: right;\">  0.354246</td><td style=\"text-align: right;\">  0.375413</td><td style=\"text-align: right;\">  0.599034</td><td style=\"text-align: right;\">  0.469723</td><td style=\"text-align: right;\">  0.370174</td><td style=\"text-align: right;\">  0.404859</td><td style=\"text-align: right;\">  0.373395</td><td style=\"text-align: right;\">  0.469265</td><td style=\"text-align: right;\">   0.42864 </td><td style=\"text-align: right;\">   0.623298</td><td style=\"text-align: right;\">   0.630007</td><td style=\"text-align: right;\">   0.564928</td><td style=\"text-align: right;\">   0.487995</td><td style=\"text-align: right;\">   0.451736</td><td style=\"text-align: right;\">   0.585484</td><td style=\"text-align: right;\">   0.376369</td><td style=\"text-align: right;\">   0.546255</td><td style=\"text-align: right;\">   0.435481</td><td style=\"text-align: right;\">   0.438662</td><td style=\"text-align: right;\">   0.547571</td><td style=\"text-align: right;\">   0.503024</td><td style=\"text-align: right;\">   0.414935</td><td style=\"text-align: right;\">   0.472686</td><td style=\"text-align: right;\">   0.545484</td><td style=\"text-align: right;\">   0.598749</td><td style=\"text-align: right;\">   0.526426</td><td style=\"text-align: right;\">   0.564826</td><td style=\"text-align: right;\">   0.404069</td><td style=\"text-align: right;\">   0.506383</td><td style=\"text-align: right;\">   0.485353</td><td style=\"text-align: right;\">   0.438983</td><td style=\"text-align: right;\">   0.475062</td><td style=\"text-align: right;\">   0.55539 </td><td style=\"text-align: right;\">   0.529923</td><td style=\"text-align: right;\">   0.369996</td><td style=\"text-align: right;\">   0.436297</td><td style=\"text-align: right;\">   0.609394</td><td style=\"text-align: right;\">   0.570837</td><td style=\"text-align: right;\">   0.609865</td><td style=\"text-align: right;\">   0.448231</td><td style=\"text-align: right;\">   0.411113</td><td style=\"text-align: right;\">   0.514002</td><td style=\"text-align: right;\">   0.546722</td><td style=\"text-align: right;\">   0.532473</td><td style=\"text-align: right;\">   0.407406</td><td style=\"text-align: right;\">   0.586454</td><td style=\"text-align: right;\">   0.362659</td><td style=\"text-align: right;\">   0.475919</td><td style=\"text-align: right;\">   0.425869</td><td style=\"text-align: right;\">   0.516487</td><td style=\"text-align: right;\">   0.552116</td><td style=\"text-align: right;\">   0.481786</td><td style=\"text-align: right;\">   0.370648</td><td style=\"text-align: right;\">   0.364882</td><td style=\"text-align: right;\">   0.607843</td><td style=\"text-align: right;\">   0.4695  </td><td style=\"text-align: right;\">   0.663855</td><td style=\"text-align: right;\">   0.426411</td><td style=\"text-align: right;\">   0.452507</td><td style=\"text-align: right;\">  0.544231 </td><td style=\"text-align: right;\">   0.49912 </td><td style=\"text-align: right;\">   0.519766</td><td style=\"text-align: right;\">   0.38792 </td><td style=\"text-align: right;\">  0.355512 </td><td style=\"text-align: right;\">   0.493653</td><td style=\"text-align: right;\">   0.631899</td><td style=\"text-align: right;\">   0.501821</td><td style=\"text-align: right;\">   0.577798</td><td style=\"text-align: right;\">   0.447541</td><td style=\"text-align: right;\">   0.511139</td><td style=\"text-align: right;\">   0.449457</td><td style=\"text-align: right;\">   0.559603</td><td style=\"text-align: right;\">   0.593044</td><td style=\"text-align: right;\">   0.534333</td><td style=\"text-align: right;\">   0.451951</td><td style=\"text-align: right;\">   0.613922</td><td style=\"text-align: right;\">   0.527886</td><td style=\"text-align: right;\">   0.497082</td><td style=\"text-align: right;\">   0.397653</td><td style=\"text-align: right;\">   0.466497</td><td style=\"text-align: right;\">   0.561578</td><td style=\"text-align: right;\">   0.451895</td><td style=\"text-align: right;\">   0.608109</td><td style=\"text-align: right;\">   0.449011</td><td style=\"text-align: right;\">   0.475092</td><td style=\"text-align: right;\">   0.503237</td><td style=\"text-align: right;\">   0.394309</td><td style=\"text-align: right;\">   0.405247</td><td style=\"text-align: right;\">   0.411261</td><td style=\"text-align: right;\">   0.397601</td><td style=\"text-align: right;\">   0.343133</td><td style=\"text-align: right;\">   0.484967</td><td style=\"text-align: right;\">   0.551621</td><td style=\"text-align: right;\">   0.493451</td><td style=\"text-align: right;\">   0.428051</td><td style=\"text-align: right;\">   0.366979</td><td style=\"text-align: right;\">   0.49993 </td><td style=\"text-align: right;\">   0.541835</td><td style=\"text-align: right;\">    0.411495</td></tr>\n",
              "</tbody>\n",
              "</table><pre style='font-size: smaller; margin-bottom: 1em;'>[2510 rows x 100 columns]</pre>"
            ],
            "text/plain": [
              "  DF.L1.C1    DF.L1.C2    DF.L1.C3    DF.L1.C4    DF.L1.C5    DF.L1.C6    DF.L1.C7    DF.L1.C8    DF.L1.C9    DF.L1.C10    DF.L1.C11    DF.L1.C12    DF.L1.C13    DF.L1.C14    DF.L1.C15    DF.L1.C16    DF.L1.C17    DF.L1.C18    DF.L1.C19    DF.L1.C20    DF.L1.C21    DF.L1.C22    DF.L1.C23    DF.L1.C24    DF.L1.C25    DF.L1.C26    DF.L1.C27    DF.L1.C28    DF.L1.C29    DF.L1.C30    DF.L1.C31    DF.L1.C32    DF.L1.C33    DF.L1.C34    DF.L1.C35    DF.L1.C36    DF.L1.C37    DF.L1.C38    DF.L1.C39    DF.L1.C40    DF.L1.C41    DF.L1.C42    DF.L1.C43    DF.L1.C44    DF.L1.C45    DF.L1.C46    DF.L1.C47    DF.L1.C48    DF.L1.C49    DF.L1.C50    DF.L1.C51    DF.L1.C52    DF.L1.C53    DF.L1.C54    DF.L1.C55    DF.L1.C56    DF.L1.C57    DF.L1.C58    DF.L1.C59    DF.L1.C60    DF.L1.C61    DF.L1.C62    DF.L1.C63    DF.L1.C64    DF.L1.C65    DF.L1.C66    DF.L1.C67    DF.L1.C68    DF.L1.C69    DF.L1.C70    DF.L1.C71    DF.L1.C72    DF.L1.C73    DF.L1.C74    DF.L1.C75    DF.L1.C76    DF.L1.C77    DF.L1.C78    DF.L1.C79    DF.L1.C80    DF.L1.C81    DF.L1.C82    DF.L1.C83    DF.L1.C84    DF.L1.C85    DF.L1.C86    DF.L1.C87    DF.L1.C88    DF.L1.C89    DF.L1.C90    DF.L1.C91    DF.L1.C92    DF.L1.C93    DF.L1.C94    DF.L1.C95    DF.L1.C96    DF.L1.C97    DF.L1.C98    DF.L1.C99    DF.L1.C100\n",
              "----------  ----------  ----------  ----------  ----------  ----------  ----------  ----------  ----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  -----------  ------------\n",
              "  0.476524    0.351325    0.43057     0.58709     0.498779    0.351123    0.43165     0.434156    0.513435     0.425088     0.645276     0.59409      0.612404     0.492538     0.455857     0.535056     0.386099     0.551709     0.446371     0.441731     0.58972      0.486671     0.40743      0.471261     0.520296     0.603224     0.518462     0.596465     0.370875     0.557034     0.540086     0.46255      0.460741     0.573207     0.539062     0.360963     0.434825     0.559179     0.57408      0.5832       0.479673     0.425198     0.529588     0.541111     0.524724     0.492005     0.556367     0.340544     0.43685      0.396498     0.535787     0.542237     0.48506      0.366534     0.416451     0.584673     0.466162     0.63709      0.408258     0.421337    0.581714      0.562318     0.572821     0.429933    0.329379      0.490875     0.604129     0.51733      0.553564     0.491644     0.520014     0.425857     0.5132       0.557871     0.554447     0.493894     0.612963     0.506876     0.522889     0.384257     0.453536     0.620639     0.416671     0.582229     0.476221     0.488743     0.507247     0.437465     0.398904     0.434922     0.377032     0.333891     0.453917     0.521924     0.515675     0.411477     0.402156     0.518723     0.532911      0.436783\n",
              "  0.483533    0.3507      0.454352    0.582611    0.511074    0.344463    0.444425    0.46231     0.533681     0.425043     0.651939     0.577442     0.630776     0.492383     0.457919     0.512822     0.39109      0.556961     0.449402     0.446064     0.604305     0.482965     0.402923     0.468907     0.510522     0.605817     0.518786     0.609786     0.358386     0.577154     0.563524     0.473291     0.452516     0.579641     0.540821     0.35939      0.436822     0.535933     0.572102     0.569801     0.495411     0.430576     0.536577     0.535303     0.517971     0.528659     0.544529     0.332039     0.418304     0.38647      0.54159      0.53495      0.484054     0.367111     0.440062     0.574687     0.467009     0.624428     0.398106     0.405138    0.597738      0.587179     0.596047     0.447533    0.321964      0.49241      0.591987     0.524206     0.541211     0.513142     0.525888     0.41922      0.49343      0.543739     0.564217     0.509384     0.612905     0.495906     0.534107     0.377825     0.449545     0.648613     0.402344     0.568061     0.489448     0.496701     0.509203     0.455851     0.396456     0.445851     0.366975     0.330574     0.43947      0.510942     0.526586     0.405236     0.417277     0.523227     0.524733      0.447753\n",
              "  0.476719    0.21866     0.448906    0.644451    0.536158    0.193744    0.408292    0.4617      0.584611     0.352388     0.82812      0.629441     0.765838     0.491766     0.429619     0.485566     0.301043     0.592637     0.416271     0.395089     0.736464     0.449477     0.317513     0.452399     0.496157     0.681924     0.517563     0.709129     0.217015     0.683404     0.656651     0.46969      0.40631      0.668587     0.586635     0.225311     0.370952     0.528265     0.636284     0.622974     0.501973     0.381498     0.580211     0.566041     0.537395     0.6135       0.550915     0.171354     0.333916     0.267827     0.594519     0.560313     0.484369     0.242651     0.416799     0.628414     0.425928     0.693353     0.3055       0.319055    0.696461      0.710957     0.709285     0.430433    0.141121      0.479015     0.637172     0.557832     0.567066     0.541517     0.548413     0.321015     0.453319     0.545561     0.62308      0.55223      0.692383     0.485987     0.575138     0.273708     0.397425     0.812026     0.290709     0.608949     0.498399     0.499972     0.516034     0.445563     0.3101       0.410384     0.251289     0.186443     0.367814     0.490345     0.555209     0.30878      0.369205     0.565621     0.548981      0.425608\n",
              "  0.475652    0.471325    0.411169    0.535027    0.463605    0.493546    0.450426    0.404383    0.445556     0.489934     0.480707     0.564829     0.472623     0.494889     0.479216     0.582933     0.46211      0.511714     0.474626     0.481242     0.457599     0.518279     0.490551     0.489997     0.542503     0.530977     0.51634      0.492904     0.510573     0.441324     0.431688     0.454377     0.512683     0.486849     0.497362     0.482481     0.490539     0.591165     0.520454     0.550119     0.455922     0.464777     0.482622     0.521771     0.516639     0.377338     0.561814     0.494307     0.533745     0.51242      0.483746     0.529009     0.487671     0.477103     0.412336     0.546124     0.50082      0.588356     0.504501     0.518185    0.475808      0.426361     0.44599      0.427835    0.498145      0.499352     0.575671     0.479623     0.544211     0.441967     0.491978     0.51906      0.569662     0.569884     0.490172     0.44115      0.540606     0.528582     0.474093     0.48585      0.503699     0.441739     0.532001     0.561942     0.453403     0.47603      0.498869     0.428154     0.479571     0.455368     0.493249     0.467596     0.534564     0.550453     0.477501     0.50472      0.430349     0.47841      0.522615      0.445604\n",
              "  0.432472    0.844933    0.224797    0.40672     0.288238    0.975345    0.451478    0.181005    0.144282     0.70257      0            0.551071     0            0.482316     0.541771     0.842303     0.675488     0.386859     0.530074     0.606927     0            0.666616     0.754536     0.543751     0.671961     0.304288     0.544051     0.105269     1.0174       0            0            0.377443     0.693907     0.17901      0.339147     0.881968     0.675098     0.795846     0.336392     0.505437     0.319754     0.551649     0.305513     0.461062     0.495911     0            0.650957     1.01639      0.911347     0.940919     0.271759     0.500085     0.477703     0.829501     0.286115     0.483716     0.622747     0.495894     0.830116     0.872854    0.0678372     0            0            0.323815    1.08544       0.543945     0.554291     0.3297       0.564405     0.198609     0.391935     0.868632     0.847124     0.691131     0.25159      0.174414     0.324157     0.636397     0.265265     0.823606     0.690035     0            0.965746     0.547384     0.328718     0.412005     0.467289     0.302015     0.741138     0.464123     0.889139     0.898089     0.845722     0.712538     0.31423      0.829344     0.435924     0.300186     0.494526      0.414143\n",
              "  0.55441     0.567208    0.658179    0.428425    0.570888    0.531078    0.59493     0.665731    0.593789     0.531005     0.438878     0.366115     0.575121     0.521817     0.520181     0.385514     0.578406     0.496011     0.555911     0.528229     0.54356      0.464297     0.53301      0.503185     0.440284     0.484838     0.470747     0.549994     0.477135     0.580652     0.594821     0.559173     0.494552     0.494641     0.505499     0.548014     0.531433     0.381313     0.491431     0.394764     0.581734     0.570582     0.513815     0.481602     0.476893     0.677937     0.420255     0.528612     0.438105     0.474026     0.53229      0.473961     0.509224     0.555417     0.654181     0.399499     0.513376     0.415937     0.509874     0.461929    0.558411      0.608461     0.583392     0.628866    0.522576      0.490189     0.416643     0.518904     0.422044     0.604086     0.504977     0.485992     0.398004     0.410326     0.525418     0.597925     0.468515     0.450821     0.552463     0.516592     0.484799     0.5566       0.467741     0.423157     0.561207     0.526199     0.510647     0.627917     0.523427     0.586414     0.50365      0.545875     0.461171     0.431335     0.546656     0.509809     0.625828     0.537315     0.476626      0.575106\n",
              "  0.481561    0.174852    0.470563    0.659853    0.556553    0.137548    0.408519    0.488361    0.620441     0.327971     0.893121     0.630645     0.828683     0.49242      0.422323     0.455063     0.276235     0.608206     0.409172     0.381377     0.795365     0.433352     0.285937     0.445483     0.481392     0.708885     0.515728     0.754613     0.158015     0.738456     0.710043     0.478834     0.384205     0.704368     0.604345     0.179115     0.350234     0.503717     0.656747     0.627828     0.518618     0.371057     0.601126     0.571933     0.538638     0.677003     0.540889     0.110294     0.289008     0.218295     0.618548     0.562733     0.484621     0.201822     0.431872     0.635975     0.412377     0.703889     0.266433     0.276409    0.744345      0.776637     0.768747     0.442368    0.0733437     0.474845     0.63989      0.575464     0.564157     0.570807     0.560721     0.281063     0.420819     0.531795     0.6514       0.582558     0.71801      0.472779     0.599554     0.233791     0.375951     0.891497     0.239998     0.609693     0.513573     0.508043     0.519957     0.460285     0.279453     0.409376     0.204323     0.136011     0.33085      0.472026     0.574721     0.270899     0.368423     0.58537      0.55079       0.429191\n",
              "  0.480565    0.180871    0.466495    0.658026    0.553186    0.145602    0.407966    0.483548    0.614698     0.331387     0.883778     0.631198     0.819128     0.492189     0.423235     0.460224     0.279415     0.60606      0.409868     0.383245     0.786421     0.435952     0.290346     0.446407     0.483939     0.70515      0.51623      0.747783     0.166748     0.729903     0.701673     0.477125     0.387452     0.699117     0.601707     0.185623     0.353134     0.508044     0.653823     0.627664     0.515769     0.372162     0.597974     0.57117      0.538547     0.666654     0.542897     0.119056     0.29586      0.225686     0.61483      0.562532     0.484463     0.207509     0.428805     0.635421     0.41434      0.702986     0.272046     0.282767    0.737075      0.766367     0.759599     0.439872    0.0831707     0.475529     0.640081     0.57276      0.565002     0.565992     0.558927     0.287058     0.426191     0.53441      0.647176     0.577519     0.714562     0.474952     0.595719     0.239462     0.379176     0.879564     0.247624     0.610051     0.510989     0.506725     0.519342     0.45739      0.283746     0.40905      0.211087     0.143057     0.336478     0.475165     0.571655     0.276401     0.367803     0.582213     0.550623      0.428189\n",
              "  0.464251    0.312435    0.395287    0.613949    0.488761    0.316773    0.404748    0.398033    0.502955     0.405451     0.685048     0.631566     0.624262     0.488451     0.44471      0.557049     0.352588     0.561101     0.428508     0.425642     0.603133     0.488013     0.384796     0.466044     0.531776     0.62449      0.524953     0.607852     0.348199     0.558223     0.53558      0.447393     0.454031     0.589294     0.546586     0.326067     0.416653     0.586565     0.590216     0.614882     0.46375      0.400948     0.533785     0.552272     0.533636     0.466893     0.577325     0.304659     0.43351      0.379008     0.539091     0.554477     0.481868     0.331708     0.378567     0.615449     0.456678     0.674237     0.389269     0.412093    0.589459      0.560927     0.575851     0.398719    0.291233      0.489852     0.634949     0.518442     0.575316     0.474958     0.522779     0.411696     0.529182     0.580434     0.56111      0.480234     0.638437     0.515529     0.520001     0.359695     0.446163     0.636775     0.403944     0.609076     0.463204     0.482622     0.506996     0.407799     0.376078     0.409928     0.352973     0.295099     0.450024     0.534612     0.511784     0.39211      0.365739     0.518444     0.543532      0.414541\n",
              "  0.459919    0.354246    0.375413    0.599034    0.469723    0.370174    0.404859    0.373395    0.469265     0.42864      0.623298     0.630007     0.564928     0.487995     0.451736     0.585484     0.376369     0.546255     0.435481     0.438662     0.547571     0.503024     0.414935     0.472686     0.545484     0.598749     0.526426     0.564826     0.404069     0.506383     0.485353     0.438983     0.475062     0.55539      0.529923     0.369996     0.436297     0.609394     0.570837     0.609865     0.448231     0.411113     0.514002     0.546722     0.532473     0.407406     0.586454     0.362659     0.475919     0.425869     0.516487     0.552116     0.481786     0.370648     0.364882     0.607843     0.4695       0.663855     0.426411     0.452507    0.544231      0.49912      0.519766     0.38792     0.355512      0.493653     0.631899     0.501821     0.577798     0.447541     0.511139     0.449457     0.559603     0.593044     0.534333     0.451951     0.613922     0.527886     0.497082     0.397653     0.466497     0.561578     0.451895     0.608109     0.449011     0.475092     0.503237     0.394309     0.405247     0.411261     0.397601     0.343133     0.484967     0.551621     0.493451     0.428051     0.366979     0.49993      0.541835      0.411495\n",
              "[2510 rows x 100 columns]\n"
            ]
          },
          "execution_count": 503,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hidden_layer_output = best_dl_model.deepfeatures(test_h2o, layer=0)\n",
        "hidden_layer_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBuI4EjUS1Mv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}